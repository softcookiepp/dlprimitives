
/// Autogenerated, don't edit
#include <map>
#include <string>
namespace dlprim {
namespace gpu {
std::map<std::string,std::string> kernel_sources = {
        {"activation", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 



__kernel
void activation(ulong size,__global dtype *a,ulong a_offset, __global dtype *c,ulong c_offset)
{
    ulong pos = get_global_id(0);
    if(pos >= size)
        return;
    a+=a_offset;
    c+=c_offset;
    c[pos] = ACTIVATION_F(a[pos]);
}


__kernel
void activation_diff(ulong size,__global dtype *y,ulong y_offset, __global dtype *dy,ulong dy_offset,__global dtype *dx,ulong dx_offset,dtype beta)
{
    ulong pos = get_global_id(0);
    if(pos >= size)
        return;
    y+=y_offset;
    dy+=dy_offset;
    dx+=dx_offset;
    dtype y_val  = y[pos];
    dtype dy_val = dy[pos];
    dtype diff = ACTIVATION_FINV(y_val,dy_val);
    if(beta == 0)
        dx[pos] = diff;
    else
        dx[pos] = mad(dx[pos],beta,diff);
}


)kern_src" },{"axpby", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


__kernel
void axpby(ulong size,dtype a,__global const dtype *x,ulong x_off,dtype b,__global const dtype *y,ulong y_off,__global dtype *z,ulong z_off )
{
    ulong pos = get_global_id(0);
    if(pos >= size)
        return;
    x+=x_off;
    y+=y_off;
    z+=z_off;
    z[pos] = a*x[pos] + b*y[pos];
}
)kern_src" },{"benchmark", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#ifndef USE_HALF
#define USE_HALF 0
#endif
#if USE_HALF == 1
#pragma OPENCL EXTENSION cl_khr_fp16 : enable
typedef half vec1;
typedef half2 vec2;
typedef half4 vec4;
typedef half8 vec8;
typedef half16 vec16;
#else
typedef float vec1;
typedef float2 vec2;
typedef float4 vec4;
typedef float8 vec8;
typedef float16 vec16;
#endif

__kernel
void flops_v1(__global float *out)
{
    int i=get_global_id(0);
    vec1 x=(vec1)(0.23f);
    vec1 y=(vec1)(0.5);
    vec1 z=(vec1)(0.0000023);
    vec1 cs = (vec1)(sin(0.56*i));
    vec1 sn = (vec1)(cos(0.56*i));

    #pragma unroll(100)
    for(int i=0;i<10000;i++) {
        vec1 xx = x*cs - (y*sn+z);
        vec1 yy = x*sn + (y*cs-z);
        x=xx;
        y=yy;
    }
    out[get_global_id(0)]=x*y;
}


__kernel
void flops_v2(__global float *out)
{
    int i=get_global_id(0);
    vec2 x=(vec2)(0.23f,0.21);
    vec2 y=(vec2)(0,0.5);
    vec2 z=(vec2)(0.00001,0.0000000023);
    vec2 cs = (vec2)(sin(0.56*i),sin(0.52*i));
    vec2 sn = (vec2)(cos(0.56*i),cos(0.52*i));

    #pragma unroll(100)
    for(int i=0;i<10000;i++) {
        vec2 xx = x*cs - (y*sn+z);
        vec2 yy = x*sn + (y*cs-z);
        x=xx;
        y=yy;
    }
    out[get_global_id(0)]=dot(x,y);
}

__kernel
void flops_v4(__global float *out)
{
    int i=get_global_id(0);
    vec4 x=(vec4)(0.23f,0.21,0.2,0.7);
    vec4 y=(vec4)(0,0.5,1.0,2.0);
    vec4 z=(vec4)(0.00001,0.000002,0.00012,0.0005);
    vec4 cs = (vec4)(sin(0.56*i),sin(0.52*i),sin(0.5*i),sin(0.1*i));
    vec4 sn = (vec4)(cos(0.56*i),cos(0.52*i),cos(0.5*i),cos(0.1*i));

    #pragma unroll(100)
    for(int i=0;i<10000;i++) {
        vec4 xx = x*cs - (y*sn+z);
        vec4 yy = x*sn + (y*cs-z);
        x=xx;
        y=yy;
    }
    out[get_global_id(0)]=dot(x,y);
}

__kernel
void flops_v8(__global float *out)
{
    int i=get_global_id(0);
    vec8 x=(vec8)(0.23f,0.21,0.2,0.7,   0.1,0.11,0.12,0.13);
    vec8 y=(vec8)(0,0.5,1.0 ,2.0,       0.21,0.22,0.23,0.25);
    vec8 z=(vec8)(0.00001,0.000002,0.00012,0.0005, 0.00001,01.0000021,0.000121,0.00051);
    vec8 cs = (vec8)(sin(0.56*i),sin(0.52*i),sin(0.5*i),sin(0.1*i),sin(0.561*i),sin(0.521*i),sin(0.51*i),sin(0.11*i));
    vec8 sn = (vec8)(cos(0.56*i),cos(0.52*i),cos(0.5*i),cos(0.1*i),cos(0.561*i),cos(0.521*i),cos(0.51*i),cos(0.11*i));

    #pragma unroll(100)
    for(int i=0;i<10000;i++) {
        vec8 xx = x*cs - (y*sn+z);
        vec8 yy = x*sn + (y*cs-z);
        x=xx;
        y=yy;
    }
    out[get_global_id(0)]=dot(x.lo,y.lo) + dot(x.hi,y.hi);
}

__kernel
void flops_v16(__global float *out)
{
    int i=get_global_id(0);
    vec16 x=(vec16)(0.23f,0.21,0.2,0.7,   0.1,0.11,0.12,0.13,0,0.5,1.0 ,2.0,       0.21,0.22,0.23,0.25);
    vec16 y=(vec16)(0,0.5,1.0 ,2.0,       0.21,0.22,0.23,0.25,0.23f,0.21,0.2,0.7,   0.1,0.11,0.12,0.13);
    vec16 z=(vec16)(0.00001,0.000002,0.00012,0.0005, 0.00001,01.0000021,0.000121,0.00051,
                    0.000001,0.0000002,0.000012,0.00005, 0.000001,01.0000021,0.0000121,0.000051);
    vec16 cs = (vec16)(sin(0.56*i),sin(0.52*i),sin(0.5*i),sin(0.1*i),sin(0.561*i),sin(0.521*i),sin(0.51*i),sin(0.11*i),
                       sin(1.56*i),sin(1.52*i),sin(1.5*i),sin(1.1*i),sin(1.561*i),sin(1.521*i),sin(1.51*i),sin(1.11*i) );
    vec16 sn = (vec16)(cos(0.56*i),cos(0.52*i),cos(0.5*i),cos(0.1*i),cos(0.561*i),cos(0.521*i),cos(0.51*i),cos(0.11*i),
                       cos(1.56*i),cos(1.52*i),cos(1.5*i),cos(1.1*i),cos(1.561*i),cos(1.521*i),cos(1.51*i),cos(1.11*i) );

    #pragma unroll(100)
    for(int i=0;i<10000;i++) {
        vec16 xx = x*cs - (y*sn+z);
        vec16 yy = x*sn + (y*cs-z);
        x=xx;
        y=yy;
    }
    out[get_global_id(0)]=dot(x.lo.lo,y.lo.lo) + dot(x.lo.hi,y.lo.hi) + dot(x.hi.lo,y.hi.lo) + dot(x.hi.hi,y.hi.hi);

}

__kernel
void memspeed_v1(__global int *p)
{
    p += get_global_id(0);
    *p += 1;
}

__kernel
void memspeed_v2(__global int2 *p)
{
    p += get_global_id(0);
    *p += (int2)(1);
}

__kernel
void memspeed_v4(__global int4 *p)
{
    p += get_global_id(0);
    *p += (int4)(1);
}

__kernel
void memspeed_v8(__global int8 *p)
{
    p += get_global_id(0);
    *p += (int8)(1);
}

__kernel
void memspeed_v16(__global int16 *p)
{
    p += get_global_id(0);
    *p += (int16)(1);
}


)kern_src" },{"bias", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


__kernel 
void ip_bias(int batch,int N,
        __global dtype *data,ulong data_offset,
        __global const dtype *bias,ulong bias_offset)
{
    data+=data_offset;
    bias+=bias_offset;
    int r = get_global_id(0);
    int c = get_global_id(1);
    if(r >= batch || c >= N)
        return;
    data += r*N+c;
    bias+=c;
    float v = *data + *bias;
    v=ACTIVATION_F(v);
    *data = v;
}


__kernel 
void activation_inplace(int tensor_size,__global dtype *data,ulong data_offset)
{
    data+=data_offset;
    if(get_global_id(0) >= tensor_size)
        return;
    data += get_global_id(0);
    *data = ACTIVATION_F(*data);
}

       
)kern_src" },{"bn_sums", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#ifndef my_get_local_wg_id
#define my_get_local_wg_id() ((get_local_id(2) * get_local_size(1) * get_local_size(0)) + (get_local_id(1) * get_local_size(0)) + get_local_id(0))
#endif

#define REDUCE_PREPARE_X2(WG_SIZE,dtype) __local dtype my_reduce_x2[2][WG_SIZE];
#define REDUCE_USING_OP_X2(myval,reduce_op) \
    do { \
        int lid = my_get_local_wg_id(); \
        my_reduce_x2[0][lid] = myval.s0; \
        my_reduce_x2[1][lid] = myval.s1; \
        barrier(CLK_LOCAL_MEM_FENCE); \
        const int WGS = sizeof(my_reduce_x2[0])/sizeof(my_reduce_x2[0][0]); \
        for(int i=WGS / 2;i>0; i>>= 1) { \
            if(lid < i) { \
                my_reduce_x2[0][lid] = reduce_op(my_reduce_x2[0][lid],my_reduce_x2[0][lid+i]); \
                my_reduce_x2[1][lid] = reduce_op(my_reduce_x2[1][lid],my_reduce_x2[1][lid+i]); \
            } \
            barrier(CLK_LOCAL_MEM_FENCE); \
        } \
        myval.s0= my_reduce_x2[0][0]; \
        myval.s1= my_reduce_x2[1][0]; \
    } while(0)

#define REDUCE_X2_OP_ADD(x,y) ((x) + (y))
#define REDUCE_X2_OP_MAX(x,y) max((x),(y))

#define my_work_group_reduce_add_x2(val) REDUCE_USING_OP_X2(val,REDUCE_X2_OP_ADD)
#define my_work_group_reduce_max_x2(val) REDUCE_USING_OP_X2(val,REDUCE_X2_OP_MAX)


#ifndef SECOND_REDUCE_SIZE
#define SECOND_REDUCE_SIZE 1
#endif

#ifndef BACKWARD
#define BACKWARD 0
#endif

__kernel
__attribute__((reqd_work_group_size(WG_SIZE,1,1)))
void compute(
          int batch,int channels,int HW,
          __global float const *x,ulong x_offset,
#if BACKWARD == 1
          __global float const *dy,ulong dy_offset,
          __global float *dyx_sum,ulong dyx_sum_offset,
          __global float *dy_sum,ulong dy_sum_offset
#else
    #if SECOND_REDUCE_SIZE == 1
          __global float *x_mean,ulong x_mean_offset,
          __global float *x_var, ulong x_var_offset
    #else
          __global float *x_sum, ulong x_sum_offset,
          __global float *x2_sum,ulong x2_sum_offset
    #endif          
#endif                
          )
{
    int feature = get_global_id(1);
    if(feature >= channels)
        return;

    x   += x_offset;
#if BACKWARD == 1
    dy   += dy_offset;
    dy_sum += dy_sum_offset;
    dyx_sum += dyx_sum_offset;
#else
  #if SECOND_REDUCE_SIZE == 1
    x_mean += x_mean_offset;
    x_var  += x_var_offset;
  #else
    x_sum += x_sum_offset;
    x2_sum += x2_sum_offset;
  #endif
#endif    
    
    x  += feature * HW;
#if BACKWARD == 1
    dy += feature * HW;
#endif    

    int items = batch * HW;
    const int wg_size2 = WG_SIZE * SECOND_REDUCE_SIZE;
    int items_per_wg = (items + wg_size2 - 1) / wg_size2;
    int my_start = items_per_wg * get_global_id(0); // it is same as local id for 1stage reduce
    int my_end   = min(my_start + items_per_wg,items);

    float sum1 = 0;
    float sum2 = 0;
    int b  = my_start / HW;
    int rc = my_start % HW;

    #pragma unroll(16)
    for(int index = my_start;index <my_end;index ++) {
        if(b < batch && rc < HW) {
            int pos = b*(channels * HW) + rc;
            #if BACKWARD == 1
                float xv  =   x[pos];
                float dyv =  dy[pos];
                sum1 += xv*dyv;
                sum2 += dyv;
            #else
                float val =   x[pos];
                sum1 += val;
                sum2 += val*val;
            #endif
        }
        rc++;
        if(rc == HW) {
            rc = 0;
            b ++;
        }
    }

    REDUCE_PREPARE_X2(WG_SIZE,float);

    float2 sums=(float2)(sum1,sum2);

    my_work_group_reduce_add_x2(sums);
    sum1 = sums.s0;
    sum2 = sums.s1;

    if(get_local_id(0) == 0) {
        #if SECOND_REDUCE_SIZE == 1
            int pos = feature;
        #else
            int pos = feature + channels * get_group_id(0);
        #endif
        #if BACKWARD == 1
            dyx_sum[pos] = sum1;
             dy_sum[pos] = sum2;
        #else
            #if SECOND_REDUCE_SIZE == 1
            float mean_val  = sum1 / (batch * HW);
            float mean2_val = sum2 / (batch * HW);
            x_mean[pos] = mean_val;
             x_var[pos] = mean2_val - mean_val*mean_val;
            #else
             x_sum[pos] = sum1;
            x2_sum[pos] = sum2;
            #endif
        #endif
    }
}

#if SECOND_REDUCE_SIZE > 1
__kernel
__attribute__((reqd_work_group_size(SECOND_REDUCE_SIZE,1,1)))
void reduce(int channels,
            __global float const * restrict s1,ulong s1_offset,
            __global float const * restrict s2,ulong s2_offset,
#if BACKWARD == 1
            __global float *dyx_sum,ulong dyx_sum_offset,
            __global float *dy_sum,ulong dy_sum_offset
#else
            __global float *x_mean,ulong x_mean_offset,
            __global float *x_var, ulong x_var_offset,
            float one_div_M
#endif
      )      
{
    s1 += s1_offset;
    s2 += s2_offset;
#if BACKWARD == 1
    dyx_sum += dyx_sum_offset;
    dy_sum  += dy_sum_offset;
#else
    x_mean += x_mean_offset;
    x_var  += x_var_offset;
#endif

    int f = get_global_id(1);
    if(f >= channels)
        return;
    
    REDUCE_PREPARE_X2(SECOND_REDUCE_SIZE,float);

    int read_pos = f + get_local_id(0) * channels;
    float2 sum;
    sum.s0 = s1[read_pos];
    sum.s1 = s2[read_pos];
    
    my_work_group_reduce_add_x2(sum);

    if(get_local_id(0) == 0) {
        #if BACKWARD == 0
        float mean_val  = sum.s0 * one_div_M;
        float mean2_val = sum.s1 * one_div_M;
        x_mean[f] = mean_val;
         x_var[f] = mean2_val - mean_val*mean_val;
        #else
        dyx_sum[f] = sum.s0;
         dy_sum[f] = sum.s1;
        #endif
    }    
}

#endif
)kern_src" },{"bn_utils", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
__kernel
void update_sums(int N,
                 __global float const * restrict cur_mean,ulong  cur_mean_offset,
                 __global float const * restrict cur_var ,ulong  cur_var_offset,
                 __global float * restrict run_mean,ulong  run_mean_offset,
                 __global float * restrict run_var ,ulong  run_var_offset,
                 float cur_mean_factor,float run_mean_factor,
                 float cur_var_factor, float run_var_factor)
{
    int p = get_global_id(0);
    if(p >= N)
        return;
    run_mean += run_mean_offset;
    run_var  += run_var_offset;
    cur_mean += cur_mean_offset;
    cur_var  += cur_var_offset;

    run_mean[p] = cur_mean[p] * cur_mean_factor + run_mean[p] * run_mean_factor;
    run_var[p]  = cur_var[p]  * cur_var_factor  + run_var[p]  * run_var_factor;
}



__kernel
void var_gamma_to_a(int N,float eps,
              __global float const * var, ulong  var_offset,
              __global float const * gamma, ulong  gamma_offset,
              __global float *a,ulong  a_offset)
{
    int pos = get_global_id(0);
    if(pos >= N)
        return;
    var  += var_offset;
    gamma += gamma_offset;
    a += a_offset;
    float scale = 1.0f / sqrt(var[pos] + eps);
    if(gamma)
        scale *= gamma[pos];
    a[pos] = scale;
}




__kernel
void mean_var_to_a_b(int N,float eps,
                     __global float const * mean,ulong  mean_offset,
                     __global float const * var, ulong  var_offset,
                     __global float *a,ulong  a_offset,
                     __global float *b,ulong  b_offset)
{
    int pos = get_global_id(0);
    if(pos >= N)
        return;
    mean += mean_offset;
    var  += var_offset;
    a += a_offset;
    b += b_offset;
    float scale = 1.0f / sqrt(var[pos] + eps);
    float offset  = - mean[pos] * scale;
    a[pos] = scale;
    b[pos] = offset;
}

__kernel
void combine_mean_var_with_gamma_beta(
                     int N,float eps,
                     __global float const * mean,ulong  mean_offset,
                     __global float const * var, ulong  var_offset,
                     __global float const * gamma,ulong  gamma_offset,
                     __global float const * beta,ulong  beta_offset,
                     __global float *a,ulong  a_offset,
                     __global float *b,ulong  b_offset)
{
    int pos = get_global_id(0);
    if(pos >= N)
        return;
    mean += mean_offset;
    var  += var_offset;
    gamma += gamma_offset;
    beta += beta_offset;
    a += a_offset;
    b += b_offset;
    float scale = 1.0f / sqrt(var[pos] + eps);
    float offset  = - mean[pos] * scale;
    float G = gamma[pos];
    scale *= G;
    offset = offset * G + beta[pos];
    a[pos] = scale;
    b[pos] = offset;
}

__kernel
void compute_backward_factors(int N,int M,float eps,
                              __global float const *mean,ulong  mean_offset,
                              __global float const *varrstd, ulong  varrstd_offset,
                              __global float const *dy_sum, ulong  dy_sum_offset,
                              __global float const *dyx_sum,ulong  dyx_sum_offset,
                              __global float const *gamma_in,ulong  gamma_in_offset,
                              __global float *x_factor,ulong  x_factor_offset,
                              __global float *dy_factor,ulong  dy_factor_offset,
                              __global float *offset,ulong  offset_offset)
{
    int i = get_global_id(0);
    if(i >= N)
        return;
    x_factor += x_factor_offset;
    dy_factor += dy_factor_offset;
    offset += offset_offset; 
    mean += mean_offset;
    varrstd  += varrstd_offset;
    if(gamma_in)
        gamma_in += gamma_in_offset;
    dyx_sum += dyx_sum_offset;
    dy_sum  += dy_sum_offset;

    float one_by_M = 1.0f / M;
    float rsqrtsig;
    if(eps < 0)
        rsqrtsig = varrstd[i];
    else
        rsqrtsig = 1.0f / sqrt(varrstd[i] + eps);

    float gamma=1.0f;
    if(gamma_in)
        gamma = gamma_in[i];
    float mu = mean[i];
    float dys = dy_sum[i];
    float dsig = -0.5 * gamma * (dyx_sum[i] - mu * dys) * (rsqrtsig * rsqrtsig * rsqrtsig);
    float gamma_div_sigsqrt = gamma * rsqrtsig;
    float dmu = -dys * gamma_div_sigsqrt;
    float F_dy = gamma_div_sigsqrt;
    float F_x  = 2*dsig * one_by_M;
    float B = one_by_M * (dmu - dsig * 2 * mu);

    dy_factor[i] = F_dy;
    x_factor[i] = F_x;
    offset[i] = B;
}

#define DIM_B  2
#define DIM_F  1
#define DIM_RC 0

__kernel
void forward(int batches,int channels,int HW,
             __global float const *x,ulong  x_offset,
             __global float *y,      ulong  y_offset,
             __global float const *A,ulong A_offset,
             __global float const *B,ulong B_offset)
{
    int b  = get_global_id(DIM_B);
    int f  = get_global_id(DIM_F);
    int rc = get_global_id(DIM_RC);
    if(b >= batches || f >= channels || rc >= HW)
        return;
    int pos = (b * channels + f) * HW + rc;
    y[y_offset + pos] = x[x_offset + pos] * A[A_offset + f] + B[B_offset + f];
}

__kernel
void backward_test(int batches,int channels,int HW,
             __global float *dx,ulong  dx_offset,
             __global float const *dy,ulong  dy_offset,
             __global float const *a,ulong  a_offset,
             float factor)
{
    int b  = get_global_id(DIM_B);
    int f  = get_global_id(DIM_F);
    int rc = get_global_id(DIM_RC);
    if(b >= batches || f >= channels || rc >= HW)
        return;
    dx+=dx_offset;
    dy+=dy_offset;
    a+=a_offset;
    int pos = (b * channels + f) * HW + rc;
    float val = dy[pos] * a[f];
    if(factor == 0)
        dx[pos] = val;
    else
        dx[pos] = dx[pos]*factor + val;
}


__kernel
void backward_data(int batches,int channels,int HW,
             __global float const *x,  ulong  x_offset,
             __global float const *dy, ulong  dy_offset,
             __global float const *fx, ulong  fx_offset,
             __global float const *fdy,ulong  fdy_offset,
             __global float const *b,  ulong  b_offset,
             __global float *dx,       ulong  dx_offset,
             float factor)
{
    int batch  = get_global_id(DIM_B);
    int f  = get_global_id(DIM_F);
    int rc = get_global_id(DIM_RC);
    if(batch >= batches || f >= channels || rc >= HW)
        return;
    int pos = (batch * channels + f) * HW + rc;
    float grad =  fx[fx_offset + f] * x[x_offset + pos]  + fdy[fdy_offset + f] * dy[dy_offset + pos] + b[b_offset + f];
    if(factor == 0)
        dx[dx_offset + pos] = grad;
    else
        dx[dx_offset + pos] = dx[dx_offset + pos] * factor + grad;
}

__kernel
void backward_filter(int N,
                    __global float const *mean,ulong  mean_offset,
                    __global float const *var, ulong  var_offset,
                    __global float const *dy_sum, ulong  dy_sum_offset,
                    __global float const *dyx_sum,ulong  dyx_sum_offset,
                    __global float *dgamma,ulong  dgamma_offset,
                    __global float *dbeta,ulong  dbeta_offset,
                    float eps,
                    float factor_gamma,
                    float factor_beta)
{
    int i=get_global_id(0);
    if(i >= N)
        return;
    mean += mean_offset;
    var  += var_offset;
    dy_sum += dy_sum_offset;
    dyx_sum += dyx_sum_offset;

    float dys = dy_sum[i];

    if(dgamma) {
        dgamma += dgamma_offset;
        float dG = (dyx_sum[i] - mean[i]*dys) / sqrt(var[i] + eps); 
        if(factor_gamma == 0)
            dgamma[i] = dG;
        else
            dgamma[i] = dgamma[i]*factor_gamma + dG;
    }
    
    if(dbeta) {
        dbeta += dbeta_offset;
        if(factor_beta == 0)
            dbeta[i] = dys;
        else
            dbeta[i] = dbeta[i] * factor_beta + dys;
    }
}
)kern_src" },{"bwd_bias", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#ifndef CUSTOM_REDUCE
#define CUSTOM_REDUCE 0
#endif

#define my_get_local_wg_id() ((get_local_id(2) * get_local_size(1) * get_local_size(0)) + (get_local_id(1) * get_local_size(0)) + get_local_id(0))
#if __OPENCL_VERSION__ >= 200 && !CUSTOM_REDUCE
#define REDUCE_PREPARE(WG_SIZE,dtype) do {} while(0)
#define my_work_group_reduce_add(val) do { val = work_group_reduce_add(val); } while(0)
#define my_work_group_reduce_max(val) do { val = work_group_reduce_max(val); } while(0)
#else

#define REDUCE_PREPARE(WG_SIZE,dtype) __local dtype my_reduce[WG_SIZE];
#define REDUCE_USING_OP(myval,reduce_op) \
    do { \
        int lid = my_get_local_wg_id(); \
        my_reduce[lid] = myval; \
        barrier(CLK_LOCAL_MEM_FENCE); \
        const int WGS = sizeof(my_reduce)/sizeof(my_reduce[0]); \
        for(int i=WGS / 2;i>0; i>>= 1) { \
            if(lid < i) { \
                my_reduce[lid] = reduce_op(my_reduce[lid],my_reduce[lid+i]); \
            } \
            barrier(CLK_LOCAL_MEM_FENCE); \
        } \
        myval = my_reduce[0]; \
    } while(0)

#define REDUCE_OP_ADD(x,y) ((x) + (y))
#define REDUCE_OP_MAX(x,y) max((x),(y))

#define my_work_group_reduce_add(val) REDUCE_USING_OP(val,REDUCE_OP_ADD)
#define my_work_group_reduce_max(val) REDUCE_USING_OP(val,REDUCE_OP_MAX)

#endif

#ifndef WG_SIZE
#define WG_SIZE 256
#endif

#ifndef ITEMS_PER_WI
#define ITEMS_PER_WI 1
#endif

#ifndef SIZE_2D
#define SIZE_2D 1
#endif


__kernel 
__attribute__((reqd_work_group_size(WG_SIZE,1,1)))
void bwd_bias(int features,int over,__global dtype *dy,ulong dy_offset,__global dtype *dx,ulong dx_offset,int dx_stride,float beta)
{
    dy += dy_offset;
    dx += dx_offset;
    
    int feature = get_global_id(1);
    if(feature >= features)
        return;

    int dx_pos = get_group_id(0);
    if(dx_pos >= dx_stride)
        return;

    int position   = get_global_id(0) * ITEMS_PER_WI;
    
    REDUCE_PREPARE(WG_SIZE,dtype);

    dtype val = 0;
    int batch_scale = features * SIZE_2D;
    #pragma unroll
    for(int i=0;i<ITEMS_PER_WI;i++) {
        int index = position + i;
        if(index >= over)
            continue;

        int batch = index / SIZE_2D;
        int rcpos = index % SIZE_2D;
        val += dy[batch * batch_scale + feature * SIZE_2D + rcpos];
    }
    
    my_work_group_reduce_add(val);

    int pos = feature * dx_stride + dx_pos;
    
    if(get_local_id(0) == 0) {
        if(beta == 0)
            dx[pos] = val;
        else
            dx[pos] = dx[feature] * beta + val;
    }
}

)kern_src" },{"copy", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 



__kernel void copy(ulong slice,
                   ulong dim0,
                   ulong dim1_tgt,ulong dim1_tgt_offset,
                   ulong dim1_src,ulong dim1_src_offset,
                   ulong dim2,
                   __global dtype *target, ulong target_offset,
                   __global dtype const *source, ulong source_offset,
                   dtype scale)
{
    ulong p0 = get_global_id(0);
    ulong p1 = get_global_id(1);
    ulong p2 = get_global_id(2);
    if(p0 >= dim2 || p1 >= slice || p2 >= dim0)
        return;
    source += source_offset;
    target += target_offset;
    target += p0 + (p1 + dim1_tgt_offset) * dim2 + p2 * (dim1_tgt * dim2);
    source += p0 + (p1 + dim1_src_offset) * dim2 + p2 * (dim1_src * dim2);
    if(scale == 0.0)
        *target = *source;
    else
        *target = scale * *target + *source;
}
)kern_src" },{"copy_strided", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 



__kernel void copy(
                ulong d0,ulong s0,ulong t0,
#if DIMS >= 2
                ulong d1,ulong s1,ulong t1,
#endif
#if DIMS >= 3
                ulong d2,ulong s2,ulong t2,
#endif          
#if DIMS >= 4
                ulong d3,ulong s3,ulong t3,
#endif          
#if DIMS >= 5
                ulong d4,ulong s4,ulong t4,
#endif
#if DIMS >= 6
                ulong d5,ulong s5,ulong t5,
#endif
#if DIMS >= 7
                ulong d6,ulong s6,ulong t6,
#endif
#if DIMS >= 8
                ulong d7,ulong s7,ulong t7,
#endif
                __global dtype_src const *src,ulong src_offset,
                __global dtype_tgt *tgt,ulong tgt_offset)
{
        src+=src_offset;
        tgt+=tgt_offset;
        #if DIMS == 1
            ulong i0 = get_global_id(0);
            if(i0 >= d0)
                return;
            tgt[i0*t0] = src[i0*s0];
        #elif DIMS == 2
            ulong i1 = get_global_id(0);
            ulong i0 = get_global_id(1);
            if(i0 >= d0)
                return;
            if(i1 >= d1)
                return;
            tgt[i0*t0 + i1*t1] = src[i0*s0 + i1*s1];
        #elif DIMS == 3            
            ulong i2 = get_global_id(0);
            ulong i1 = get_global_id(1);
            ulong i0 = get_global_id(2);
            if(i0 >= d0)
                return;
            if(i1 >= d1)
                return;
            if(i2 >= d2)
                return;
            tgt[i0*t0 + i1*t1 + i2*t2] = src[i0*s0 + i1*s1 + i2*s2];
        #elif DIMS == 4
            ulong ic = get_global_id(0);
            ulong i1 = get_global_id(1);
            ulong i0 = get_global_id(2);
            if(i0 >= d0)
                return;
            if(i1 >= d1)
                return;
            if(ic >= d2*d3)
                return;
            ulong i2 = ic / d3;
            ulong i3 = ic % d3;
            tgt[i0*t0 + i1*t1 + i2*t2 + i3*t3] = src[i0*s0 + i1*s1 + i2*s2 + i3*s3];
        #elif DIMS == 5
            ulong i34 = get_global_id(0);
            ulong i12 = get_global_id(1);
            ulong i0 = get_global_id(2);
            if(i0 >= d0)
                return;
            if(i12 >= d1*d2)
                return;
            if(i34 >= d3*d4)
                return;
            ulong i1  = i12 / d2;
            ulong i2  = i12 % d2;
            ulong i3  = i34 / d4;
            ulong i4  = i34 % d4;
            tgt[i0*t0 + i1*t1 + i2*t2 + i3*t3 + i4*t4] = src[i0*s0 + i1*s1 + i2*s2 + i3*s3 + i4*s4];
        #elif DIMS == 6
            ulong i45 = get_global_id(0);
            ulong i23 = get_global_id(1);
            ulong i01 = get_global_id(2);
            if(i01 >= d0*d1)
                return;
            if(i23 >= d2*d3)
                return;
            if(i45 >= d4*d5)
                return;
            ulong i0  = i01 / d1;
            ulong i1  = i01 % d1;
            ulong i2  = i23 / d3;
            ulong i3  = i23 % d3;
            ulong i4  = i45 / d5;
            ulong i5  = i45 % d5;

            tgt[i0*t0 + i1*t1 + i2*t2 + i3*t3 + i4*t4 + i5*t5] = src[i0*s0 + i1*s1 + i2*s2 + i3*s3 + i4*s4 + i5*s5];
        #elif DIMS == 7
            ulong i456 = get_global_id(0);
            ulong i23  = get_global_id(1);
            ulong i01  = get_global_id(2);
            if(i01 >= d0*d1)
                return;
            if(i23 >= d2*d3)
                return;
            if(i456 >= d4*d5*d6)
                return;
            ulong i0  = i01 / d1;
            ulong i1  = i01 % d1;
            ulong i2  = i23 / d3;
            ulong i3  = i23 % d3;
            ulong i4  = i456 / (d5*d6);
            ulong i56 = i456 % (d5*d6);
            ulong i5  = i56 / d6;
            ulong i6  = i56 % d6;

            tgt[i0*t0 + i1*t1 + i2*t2 + i3*t3 + i4*t4 + i5*t5 + i6*t6] = src[i0*s0 + i1*s1 + i2*s2 + i3*s3 + i4*s4 + i5*s5 + i6*s6];
        #elif DIMS == 8
            ulong i567 = get_global_id(0);
            ulong i234 = get_global_id(1);
            ulong i01  = get_global_id(2);
            if(i01 >= d0*d1)
                return;
            if(i234 >= d2*d3*d4)
                return;
            if(i567 >= d5*d6*d7)
                return;
            ulong i0  = i01 / d1;
            ulong i1  = i01 % d1;

            ulong i2  = i234 / (d3*d4);
            ulong i34 = i234 % (d3*d4);
            ulong i3  = i34 / d4;
            ulong i4  = i34 % d4;

            ulong i5  = i567 / (d6*d7);
            ulong i67 = i567 % (d6*d7);
            ulong i6  = i67 / d7;
            ulong i7  = i67 % d7;

            tgt[i0*t0 + i1*t1 + i2*t2 + i3*t3 + i4*t4 + i5*t5 + i6*t6 + i7*t7] = src[i0*s0 + i1*s1 + i2*s2 + i3*s3 + i4*s4 + i5*s5 + i6*s6 + i7*s7];
        #else
        #error "Unsupported dims"
        #endif
}


)kern_src" },{"depthwise_separable_bw_filter", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#ifndef CUSTOM_REDUCE
#define CUSTOM_REDUCE 0
#endif

#define my_get_local_wg_id() ((get_local_id(2) * get_local_size(1) * get_local_size(0)) + (get_local_id(1) * get_local_size(0)) + get_local_id(0))
#if __OPENCL_VERSION__ >= 200 && !CUSTOM_REDUCE
#define REDUCE_PREPARE(WG_SIZE,dtype) do {} while(0)
#define my_work_group_reduce_add(val) do { val = work_group_reduce_add(val); } while(0)
#define my_work_group_reduce_max(val) do { val = work_group_reduce_max(val); } while(0)
#else

#define REDUCE_PREPARE(WG_SIZE,dtype) __local dtype my_reduce[WG_SIZE];
#define REDUCE_USING_OP(myval,reduce_op) \
    do { \
        int lid = my_get_local_wg_id(); \
        my_reduce[lid] = myval; \
        barrier(CLK_LOCAL_MEM_FENCE); \
        const int WGS = sizeof(my_reduce)/sizeof(my_reduce[0]); \
        for(int i=WGS / 2;i>0; i>>= 1) { \
            if(lid < i) { \
                my_reduce[lid] = reduce_op(my_reduce[lid],my_reduce[lid+i]); \
            } \
            barrier(CLK_LOCAL_MEM_FENCE); \
        } \
        myval = my_reduce[0]; \
    } while(0)

#define REDUCE_OP_ADD(x,y) ((x) + (y))
#define REDUCE_OP_MAX(x,y) max((x),(y))

#define my_work_group_reduce_add(val) REDUCE_USING_OP(val,REDUCE_OP_ADD)
#define my_work_group_reduce_max(val) REDUCE_USING_OP(val,REDUCE_OP_MAX)

#endif

#ifndef SECOND_REDUCE_SIZE
#define SECOND_REDUCE_SIZE 1
#endif

__kernel
__attribute__((reqd_work_group_size(WG_SIZE,1,1)))
void conv_bw_filter(
          int batch,int height,int width,
          __global float const *input,ulong input_offset,
          __global float *kern,ulong kernel_offset,
          __global float const *output,ulong output_offset
#if SECOND_REDUCE_SIZE == 1
          ,float factor
#endif          
          )
{
    input   += input_offset;
    output  += output_offset;
    kern    += kernel_offset;

    int k = get_global_id(1);
    if( k > KERN * KERN * CHANNELS)
        return;

    int dk = k % (KERN * KERN);
    int d  = k / (KERN * KERN); 

    int dr = dk / KERN;
    int dc = dk % KERN;

    input  += d * (width * height);
    output += d * (width * height);

    int items = batch * (width * height);
    const int wg_size2 = WG_SIZE * SECOND_REDUCE_SIZE;
    int items_per_wg = (items + wg_size2 - 1) / wg_size2;
    int my_start = items_per_wg * get_global_id(0); // it is same as local id for 1stage reduce
    int my_end   = min(my_start + items_per_wg,items);

    float sum = 0;
    int b  = my_start / (width * height);
    int rc = my_start % (width * height);
    int r = rc / width;
    int c = rc % width;

    #pragma unroll(16)
    for(int index = my_start;index <my_end;index ++) {
        int sr = r - KERN/2 + dr;
        int sc = c - KERN/2 + dc;
        if(b < batch && 0<=sr && sr < height && 0 <= sc && sc < width) {
            float y = output[b*(CHANNELS * height * width) + r  * width +c ];
            float x =  input[b*(CHANNELS * height * width) + sr * width +sc];
            sum += x*y;
        }
        c++;
        if(c == width) {
            c = 0;
            r++;
            if(r == height) {
                r = 0;
                b ++;
            }
        }
    }

    REDUCE_PREPARE(WG_SIZE,float);

    my_work_group_reduce_add(sum);

    if(get_local_id(0) == 0) {
        #if SECOND_REDUCE_SIZE == 1
        if(factor == 0)
            kern[k] = sum;
        else
            kern[k] = mad(kern[k],factor,sum);
        #else
            #define STRIDE (KERN * KERN * CHANNELS)
            kern[k + STRIDE * get_group_id(0)] = sum;
        #endif
    }

}

#if SECOND_REDUCE_SIZE > 1
__kernel
__attribute__((reqd_work_group_size(SECOND_REDUCE_SIZE,1,1)))
void reduce(__global float const * restrict partial_values,ulong partial_values_offset,__global float * restrict sums,ulong sums_offset,float factor)
{
    sums += sums_offset;
    partial_values += partial_values_offset;
    int k = get_global_id(1);
    if(k > KERN * KERN * CHANNELS)
        return;
    
    REDUCE_PREPARE(SECOND_REDUCE_SIZE,float);

    float val = partial_values[k + get_local_id(0) * STRIDE];

    my_work_group_reduce_add(val);

    if(get_local_id(0) == 0) {
        if(factor == 0)
            sums[k] = val;
        else
            sums[k] = mad(sums[k],factor,val);
    }    
}

#endif
)kern_src" },{"depthwise_separable_conv", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
void atomic_addf(__global volatile float *ptr,float v)
{
#if defined(__opencl_c_ext_fp32_global_atomic_add)
    atomic_fetch_add((__global volatile atomic_float *)ptr,v);
#elif defined(cl_intel_subgroups)
    __global atomic_int *p = (__global atomic_int *)(ptr);
    int prev,newv;
    do {
        prev = atomic_load(p);
        newv = as_int(as_float(prev) + v);
    } while(! atomic_compare_exchange_weak(p,&prev,newv));
#elif defined(__NV_CL_C_VERSION)
    float prev;
    asm volatile(
        "atom.global.add.f32 %0, [%1], %2;"
        : "=f"(prev)
        : "l"(ptr) , "f"(v)
        : "memory"
    );
#else
    float oldv = *ptr;
    for(;;) {
        float newv = oldv + v;
        int prev = atomic_cmpxchg((__global volatile int *)(ptr),as_int(oldv),as_int(newv));
        if(prev == as_int(oldv))
            return;
        oldv = as_float(prev);
    }
#endif    
}


#define DIM_R 0
#define DIM_C 1
#define DIM_BD 2

#define KERN_PAD ((KERN-1)/2)

#define PATCH_H (PATCH_ROWS + KERN - 1)
#define PATCH_W (PATCH_COLS + KERN - 1)

__kernel
void conv(int batch,int height,int width,
          __global const float *input,ulong input_offset,
          __global const float *kern,ulong kernel_offset,
#if BIAS != 0
          __global const float *bias,ulong bias_offset,
#endif          
          __global float *output,ulong output_offset,
          float scale)
{
    input += input_offset;
    output += output_offset;
    kern += kernel_offset;
    int r = get_global_id(DIM_R) * PATCH_ROWS;
    int c = get_global_id(DIM_C) * PATCH_COLS;
    int b = get_global_id(DIM_BD) / CHANNELS;
    int d = get_global_id(DIM_BD) % CHANNELS;

    if(r >= height || c >= width || b >= batch)
        return;

    kern += d * KERN * KERN;
    
    input  += b * CHANNELS * width * height + d * width * height + (r - KERN_PAD) * width + c - KERN_PAD;
    output += b * CHANNELS * width * height + d * width * height + r * width + c;

    float K_vals[KERN][KERN];
    float I_vals[PATCH_H][PATCH_W];

    #pragma unroll
    for(int dr=0;dr < KERN;dr++)
        #pragma unroll
        for(int dc=0;dc<KERN;dc++)
            K_vals[dr][dc] = *kern ++;

    #if BIAS != 0
        float start_val = bias[bias_offset + d];
    #else
        const float start_val = 0;
    #endif

            

    int y = r-KERN_PAD;
    #pragma unroll
    for(int dr=0;dr<PATCH_H;dr++,y++) {
        if(y < 0 || y >= height) {
            #pragma unroll
            for(int dc=0;dc<PATCH_W;dc++)
                I_vals[dr][dc]=0;
        }
        else {
            int x = c - KERN_PAD;
            #pragma unroll
            for(int dc=0;dc<PATCH_W;dc++,x++) {
                I_vals[dr][dc]=(0 <= x && x < width) ? input[dr*width+dc] : 0;
            }
        }
    }


    #pragma unroll
    for(int dr=0;dr<PATCH_ROWS;dr++) {
        if(r+dr >= height)
            break;
        #pragma unroll
        for(int dc=0;dc<PATCH_COLS;dc++) {
            if(c+dc>=width)
                break;
            float sum = start_val;
            #pragma unroll
            for(int drk=0;drk < KERN;drk++)
                #pragma unroll
                for(int dck=0;dck<KERN;dck++)
                    sum = mad(K_vals[drk][dck],I_vals[dr+drk][dc+dck],sum);

            float value = ACTIVATION_F(sum);
            __global float *optr = output + dr*width+dc;
            if(scale == 0.0)
                *optr = value;
            else
                *optr = scale * *optr + value;
        }
    }

}

__kernel
void backward_data_conv(int batch,int height,int width,
          __global float *input,ulong input_offset,
          __global const float *kern,ulong kernel_offset,
          __global float const *output,ulong output_offset)
{
    input += input_offset;
    output += output_offset;
    kern += kernel_offset;
    int b = get_global_id(DIM_BD) / CHANNELS;
    int d = get_global_id(DIM_BD) % CHANNELS;
    int r = get_global_id(DIM_R) * PATCH_ROWS;
    int c = get_global_id(DIM_C) * PATCH_COLS;

    if(r >= height || c >= width || b >= batch)
        return;

    kern += d * KERN * KERN;
    
    input  += b * CHANNELS * width * height + d * width * height + (r - KERN_PAD) * width + c - KERN_PAD;
    output += b * CHANNELS * width * height + d * width * height + r * width + c;

    float K_vals[KERN][KERN];
    float I_vals[PATCH_H][PATCH_W] = {{0}};

    #pragma unroll
    for(int dr=0;dr < KERN;dr++)
        #pragma unroll
        for(int dc=0;dc<KERN;dc++)
            K_vals[dr][dc] = *kern ++;

            


    #pragma unroll
    for(int dr=0;dr<PATCH_ROWS;dr++) {
        if(r+dr >= height)
            break;
        #pragma unroll
        for(int dc=0;dc<PATCH_COLS;dc++) {
            if(c+dc>=width)
                break;
            float val = output[dr*width+dc];
            #pragma unroll
            for(int drk=0;drk < KERN;drk++)
                #pragma unroll
                for(int dck=0;dck<KERN;dck++)
                    I_vals[dr+drk][dc+dck] = mad(K_vals[drk][dck],val,I_vals[dr+drk][dc+dck]);

        }
    }

    int y = r-KERN_PAD;
    #pragma unroll
    for(int dr=0;dr<PATCH_H;dr++,y++) {
        if(y < 0 || y >= height) {
            #pragma unroll
            for(int dc=0;dc<PATCH_W;dc++)
                I_vals[dr][dc]=0;
        }
        if(!(y < 0 || y >= height)) {
            int x = c - KERN_PAD;
            #pragma unroll
            for(int dc=0;dc<PATCH_W;dc++,x++) {
                if(0 <= x && x < width) {
                    #if KERN == 1
                    input[dr*width+dc] += I_vals[dr][dc];
                    #else
                    atomic_addf(input + (dr*width+dc),I_vals[dr][dc]);
                    #endif
                }
            }
        }
    }
}
)kern_src" },{"eltwise", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 



#ifndef ELTOP
#define ELTOP 0
#endif

#if ELTOP == 0
#define EOP(x,y) ((x) + (y))
#elif ELTOP == 1
#define EOP(x,y) ((x) * (y))
#elif ELTOP == 2
#define EOP(x,y) max((x),(y))
#else
#error "Invaid operation"
#endif

__kernel
void eltwise(int size,__global const dtype *a,ulong  a_offset, __global const dtype *b,ulong  b_offset,__global dtype *c,ulong  c_offset,dtype c1,dtype c2)
{
    int pos = get_global_id(0);
    if(pos >= size)
        return;
    a+=a_offset;
    b+=b_offset;
    c+=c_offset;
    dtype value = EOP(a[pos]*c1,b[pos]*c2);
    c[pos] = ACTIVATION_F(value);
}

__kernel
void eltwise_bwd(  int size,
                   int da_db_select,
                    __global const dtype *a,ulong  a_offset,
                    __global dtype *da,ulong  da_offset,
                    __global const dtype *b,ulong  b_offset,
                    __global dtype *db,ulong  db_offset,
                    __global const dtype *c, ulong  c_offset,
                    __global const dtype *dc,ulong  dc_offset,
                    dtype c1,dtype c2,
                    dtype factor_a,dtype factor_b)
{
    int pos = get_global_id(0);
    if(pos >= size)
        return;
    a+=a_offset;
    b+=b_offset;
    c+=c_offset;
    da+=da_offset;
    db+=db_offset;
    dc+=dc_offset;
    dtype dy = ACTIVATION_FINV(c[pos],dc[pos]);
    if(da_db_select & 1) { // da+a
        dtype da_val;
        #if ELTOP == 0
        da_val = dy * c1;
        #elif ELTOP == 1
        da_val = dy * c1 * c2 * b[pos];
        #elif ELTOP == 2
        if(c1*a[pos] >= c2*b[pos]) 
            da_val = c1 * dy;
        else
            da_val = 0;
        #endif
        if(factor_a == 0)
            da[pos] = da_val;
        else
            da[pos] = da[pos] * factor_a + da_val;
    }
    if(da_db_select & 2) { // db+b
        dtype db_val;
        #if ELTOP == 0
        db_val = dy * c2;
        #elif ELTOP == 1
        db_val = dy * c1 * c2 * a[pos];
        #elif ELTOP == 2
        if(c1*a[pos] >= c2*b[pos])
            db_val = 0;
        else
            db_val = c2 * dy;
        #endif
        if(factor_b == 0)
            db[pos] = db_val;
        else
            db[pos] = db[pos] * factor_b + db_val;
    }
}

)kern_src" },{"fill", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


__kernel void fill(ulong total,__global dtype *p,ulong p_offset,dtype value)
{
    ulong pos = get_global_id(0);
    if(pos >= total)
        return;
    p[p_offset + pos ] = value;
}
)kern_src" },{"fwd_bias", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


__kernel void fwd_bias(int B,int F,int RC,
                       __global dtype *tensor,ulong tensor_offset,
                       __global dtype const *bias,ulong bias_offset)
{
    int rc = get_global_id(0);
    int f  = get_global_id(1);
    int b  = get_global_id(2);
    if(rc >= RC || f >= F || b >= B)
        return;
    tensor += tensor_offset;
    bias   += bias_offset;
    tensor[b * (F*RC) + f * RC + rc] += bias[f];
}
)kern_src" },{"global_pooling", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#ifndef CUSTOM_REDUCE
#define CUSTOM_REDUCE 0
#endif

#define my_get_local_wg_id() ((get_local_id(2) * get_local_size(1) * get_local_size(0)) + (get_local_id(1) * get_local_size(0)) + get_local_id(0))
#if __OPENCL_VERSION__ >= 200 && !CUSTOM_REDUCE
#define REDUCE_PREPARE(WG_SIZE,dtype) do {} while(0)
#define my_work_group_reduce_add(val) do { val = work_group_reduce_add(val); } while(0)
#define my_work_group_reduce_max(val) do { val = work_group_reduce_max(val); } while(0)
#else

#define REDUCE_PREPARE(WG_SIZE,dtype) __local dtype my_reduce[WG_SIZE];
#define REDUCE_USING_OP(myval,reduce_op) \
    do { \
        int lid = my_get_local_wg_id(); \
        my_reduce[lid] = myval; \
        barrier(CLK_LOCAL_MEM_FENCE); \
        const int WGS = sizeof(my_reduce)/sizeof(my_reduce[0]); \
        for(int i=WGS / 2;i>0; i>>= 1) { \
            if(lid < i) { \
                my_reduce[lid] = reduce_op(my_reduce[lid],my_reduce[lid+i]); \
            } \
            barrier(CLK_LOCAL_MEM_FENCE); \
        } \
        myval = my_reduce[0]; \
    } while(0)

#define REDUCE_OP_ADD(x,y) ((x) + (y))
#define REDUCE_OP_MAX(x,y) max((x),(y))

#define my_work_group_reduce_add(val) REDUCE_USING_OP(val,REDUCE_OP_ADD)
#define my_work_group_reduce_max(val) REDUCE_USING_OP(val,REDUCE_OP_MAX)

#endif

#ifndef WG_SIZE
#define WG_SIZE 256
#endif

#ifndef ITEMS_PER_WI
#define ITEMS_PER_WI 1
#endif

#ifndef ENABLE_BWD
#define ENABLE_BWD 0
#endif



__kernel 
__attribute__((reqd_work_group_size(1,WG_SIZE,1)))
void global_pooling(int items,int over,float scale,__global dtype *in,ulong  data_offset,__global dtype *out,ulong  out_offset)
{
    in += data_offset;
    out += out_offset;
    
    int b = get_global_id(0);

    if(b >= items)
        return;

    int c = get_global_id(1) * ITEMS_PER_WI;

    in += b * over;
    out += b;
    
    REDUCE_PREPARE(WG_SIZE,dtype);

#if POOL_MODE == 0

    dtype val = -DTYPE_MAX;
    for(int i=0;i<ITEMS_PER_WI;i++) {
        if(c+i < over) {
            val = max(val,in[c + i]);
        }
    }
    
    my_work_group_reduce_max(val);
#else
    dtype val = 0;
    for(int i=0;i<ITEMS_PER_WI;i++) {
        if(c+i < over) {
            val += in[c+i];
        }
    }
    
    my_work_group_reduce_add(val);
    val = val * scale;
#endif

    if(get_local_id(1) == 0)
        *out = val;
}

#if ENABLE_BWD == 1

__kernel 
__attribute__((reqd_work_group_size(1,WG_SIZE,1)))
void global_pooling_bwd(int items,int over,float scale,
                        #if POOL_MODE == 0
                        __global const dtype *x, ulong  x_offset,
                        #endif
                        __global dtype *dx,ulong  dx_offset,__global const dtype *out,ulong  out_offset,
                        dtype factor)
{
    #if POOL_MODE == 0
    x += x_offset;
    #endif
    dx += dx_offset;
    out += out_offset;
    
    int b = get_global_id(0);

    if(b >= items)
        return;

    int c = get_global_id(1) * ITEMS_PER_WI;

    #if POOL_MODE == 0
    x  += b * over;
    #endif
    dx += b * over;
    out += b;


    
    REDUCE_PREPARE(WG_SIZE,dtype);

#if POOL_MODE == 0

    dtype val = -DTYPE_MAX;
    int index = -1;
    for(int i=0;i<ITEMS_PER_WI;i++) {
        if(c+i < over) {
            dtype tmp = x[c+i];
            if(tmp > val) {
                index = c+i;
                val = tmp;
            }
        }
    }

    __local int reduce_indx[WG_SIZE];
    __local dtype reduce_vals[WG_SIZE];

    int lid = my_get_local_wg_id();
    reduce_indx[lid] = index;
    reduce_vals[lid] = val;
    barrier(CLK_LOCAL_MEM_FENCE); 
    for(int i=WG_SIZE / 2; i > 0 ; i>>=1) {
        if(lid < i) {
            dtype val;
            int ind;
            dtype vl = reduce_vals[lid];
            int   il = reduce_indx[lid];
            dtype vr = reduce_vals[lid+i];
            int   ir = reduce_indx[lid+i];
            if(vl > vr) {
                ind=il;
                val=vl;
            }
            else if(vr > vl) {
                ind=ir;
                val=vr;
            }
            else { // vr == vl
                if(il < ir) {
                    ind=il;
                    val=vl;
                }
                else {
                    ind=ir;
                    val=vr;
                }
            }
            reduce_vals[lid] = val;
            reduce_indx[lid] = ind;
        }
        barrier(CLK_LOCAL_MEM_FENCE); 
    }

    int target_index = reduce_indx[0];
    dtype store = *out;
    for(int i=0;i<ITEMS_PER_WI;i++) {
        if(c+i < over) {
            dtype val = (c + i == target_index) ? store : 0;
            if(factor == 0)
                dx[c+i] = val;
            else
                dx[c+i] = dx[c+i] * factor + val;
        }
    }
#else
    dtype store = *out * scale;
    
    for(int i=0;i<ITEMS_PER_WI;i++) {
        if(c+i < over) {
            if(factor == 0)
                dx[c+i] = store;
            else
                dx[c+i] = dx[c+i] * factor + store;
        }
    }
    
#endif

}
#endif
)kern_src" },{"im2col", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 



__kernel
__attribute__((reqd_work_group_size(1, 8, 8)))
void im2col(int batch,
            int src_rows,
            int src_cols,
            int rows,
            int cols,
            __global dtype const *img,ulong  img_offset,
            __global dtype *mat,ulong  mat_offset)
{
    img += img_offset;
    mat += mat_offset;

    int gid = get_global_id(0);
    
    int chan = gid % CHANNELS;
    int b    = gid / CHANNELS;
    int r    = get_global_id(1);
    int c    = get_global_id(2);

    if(r >= rows || c >= cols || chan >= CHANNELS || b >= batch)
        return;
    mat += CHANNELS * (KERN_H * KERN_W) * rows * cols * b;
    img += CHANNELS * src_rows * src_cols * b;
    int mat_row = r * cols + c;
    int mat_col = chan * (KERN_H * KERN_W);
    mat += mat_row * (CHANNELS * KERN_H * KERN_W) + mat_col;
    int y_pos = -PAD_H + r * STRIDE_H;
    int x_pos = -PAD_W + c * STRIDE_W;
    img += src_cols * (chan * src_rows + y_pos) + x_pos;

    #if PAD_H == 0 && PAD_W == 0
    #pragma unroll
    for(int dy = 0;dy < KERN_H * DILATE_H ;dy+= DILATE_H, img += src_cols * DILATE_H) {
        #pragma unroll
        for(int dx=0;dx < KERN_W * DILATE_W ;dx+= DILATE_W) {
            *mat++ = img[dx];
        }
    }
    #else
    #pragma unroll
    for(int dy = 0;dy < KERN_H * DILATE_H ;dy+= DILATE_H, img += src_cols * DILATE_H) {
        int y = y_pos + dy;
        #pragma unroll
        for(int dx=0;dx < KERN_W * DILATE_W ;dx+= DILATE_W) {
            int x = x_pos + dx;
            *mat++ = (y>= 0 && y < src_rows && x >= 0 && x < src_cols) ? img[dx] : 0;
        }
    }
    #endif
}
)kern_src" },{"interpolate_2d", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
void atomic_addf(__global volatile float *ptr,float v)
{
#if defined(__opencl_c_ext_fp32_global_atomic_add)
    atomic_fetch_add((__global volatile atomic_float *)ptr,v);
#elif defined(cl_intel_subgroups)
    __global atomic_int *p = (__global atomic_int *)(ptr);
    int prev,newv;
    do {
        prev = atomic_load(p);
        newv = as_int(as_float(prev) + v);
    } while(! atomic_compare_exchange_weak(p,&prev,newv));
#elif defined(__NV_CL_C_VERSION)
    float prev;
    asm volatile(
        "atom.global.add.f32 %0, [%1], %2;"
        : "=f"(prev)
        : "l"(ptr) , "f"(v)
        : "memory"
    );
#else
    float oldv = *ptr;
    for(;;) {
        float newv = oldv + v;
        int prev = atomic_cmpxchg((__global volatile int *)(ptr),as_int(oldv),as_int(newv));
        if(prev == as_int(oldv))
            return;
        oldv = as_float(prev);
    }
#endif    
}


int get_src_pos(int pos,float scale,int limit,float offset)
{
    int src_pos = (pos + offset) * scale;
    return min(src_pos,limit-1);
}
int get_tgt_pos(int pos,float scale,int limit,float offset)
{
    int tgt_pos = ceil(pos * scale - offset);
    return min(tgt_pos,limit);
}


__kernel 
void nearest_fwd(
        int N,int items_per_thread,
        int srcH,int srcW,
        int tgtH,int tgtW,
        float scale_y,float scale_x,
        float offset,
        __global const dtype* x,ulong x_offset,
        __global dtype* y,ulong y_offset)
{
    int n0 = get_global_id(2) * items_per_thread;
    int n1 = min(n0 + items_per_thread,N);
    int r = get_global_id(0);
    int c = get_global_id(1);
    if(n0 >= N || r>= tgtH || c>= tgtW)
        return;

    int step_src = srcW*srcH;
    int step_tgt = tgtW*tgtH;
    x += x_offset + n0 * step_src;
    y += y_offset + n0 * step_tgt;

    int src_r = get_src_pos(r,scale_y,srcH,offset);
    int src_c = get_src_pos(c,scale_x,srcW,offset);
    x+= src_r * srcW + src_c;
    y+= r * tgtW + c;

    for(int n=n0;n<n1;n++) {
        *y = *x;
        x += step_src;
        y += step_tgt;
    }
}

__kernel 
void nearest_bwd(
        int N,int items_per_thread,
        int srcH,int srcW,
        int tgtH,int tgtW,
        float scale_y,float scale_x,
        float offset,
        __global dtype* dx,ulong dx_offset,
        __global dtype const * dy,ulong dy_offset,float beta)
{
    int n0 = get_global_id(2) * items_per_thread;
    int n1 = min(n0 + items_per_thread,N);
    int src_r = get_global_id(0);
    int src_c = get_global_id(1);
    if(n0 >= N || src_r>= srcH || src_c>= srcW)
        return;

    int step_src = srcW*srcH;
    int step_tgt = tgtW*tgtH;
    dx += dx_offset + n0 * step_src;
    dy += dy_offset + n0 * step_tgt;

    int tgt_r0 = get_tgt_pos(src_r,  scale_y, tgtH, offset);
    int tgt_r1 = get_tgt_pos(src_r+1,scale_y, tgtH, offset);
    int tgt_c0 = get_tgt_pos(src_c,  scale_x, tgtW, offset);
    int tgt_c1 = get_tgt_pos(src_c+1,scale_x, tgtW, offset);
   
    dx += src_r * srcW + src_c;

    for(int n=n0;n<n1;n++) {
        dtype grad = 0;
        for(int r=tgt_r0;r<tgt_r1;r++) {
            for(int c=tgt_c0;c<tgt_c1;c++) {
                grad += dy[r*tgtW+c];
            }
        }
        if(beta == 0)
            *dx = grad;
        else
            *dx = beta * *dx + grad;
        dx += step_src;
        dy += step_tgt;
    }
}


float calc_lin_pos(int p,float scale,int align_corners)
{
    if(align_corners)
        return p*scale;
    return max(scale * (p+0.5f) - 0.5f,0.0f);
}

__kernel 
void bilinear(
        int fwd,
        int N,int items_per_thread,
        int srcH,int srcW,
        int tgtH,int tgtW,
        float scale_y,float scale_x,
        int align_corners,
        __global dtype* restrict x,ulong x_offset,
        __global dtype* restrict y,ulong y_offset)
{
    int n0 = get_global_id(2) * items_per_thread;
    int n1 = min(n0 + items_per_thread,N);
    int r = get_global_id(0);
    int c = get_global_id(1);
    if(n0 >= N || r>= tgtH || c>= tgtW)
        return;

    int step_src = srcW*srcH;
    int step_tgt = tgtW*tgtH;
    x += x_offset + n0 * step_src;
    y += y_offset + n0 * step_tgt;

    float src_r0f = calc_lin_pos(r,scale_y,align_corners);
    int src_r0 = src_r0f;
    int dr = (src_r0 < srcH - 1) ? 1 : 0;
    int src_r1 = src_r0 + dr;
    dtype w_r1 = src_r0f - src_r0;
    dtype w_r0 = 1 - w_r1;

    float src_c0f = calc_lin_pos(c,scale_x,align_corners);
    int src_c0 = src_c0f;
    int dc = (src_c0 < srcW - 1) ? 1 : 0;
    int src_c1 = src_c0 + dc;
    dtype w_c1 = src_c0f - src_c0;
    dtype w_c0 = 1 - w_c1;

    y+= r * tgtW + c;

    __global dtype* restrict x00 = x + src_r0 * srcW + src_c0; 
    __global dtype* restrict x01 = x + src_r0 * srcW + src_c1; 
    __global dtype* restrict x10 = x + src_r1 * srcW + src_c0; 
    __global dtype* restrict x11 = x + src_r1 * srcW + src_c1; 

    for(int n=n0;n<n1;n++) {
        if(fwd) {
            dtype val = 
                w_r0 * (w_c0 * *x00 + w_c1 * *x01) +
                w_r1 * (w_c0 * *x10 + w_c1 * *x11);
            *y = val;
        }
        else {
            dtype val = *y;
            atomic_addf(x00,w_r0 * w_c0 * val);
            atomic_addf(x01,w_r0 * w_c1 * val);
            atomic_addf(x10,w_r1 * w_c0 * val);
            atomic_addf(x11,w_r1 * w_c1 * val);
        }

        x00 += step_src;
        x01 += step_src;
        x10 += step_src;
        x11 += step_src;
        y += step_tgt;
    }
}


)kern_src" },{"nll_loss_bwd", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 



__kernel
void nll_loss_backward(int batch,int channel,
        __global dtype *dx,ulong dx_offset,
        __global itype const *label,ulong label_offset,
        __global dtype const *dy,ulong dy_offset,
        dtype scale,
        dtype factor)
{
    dx += dx_offset;
    label += label_offset;
    dy += dy_offset;
    int c = get_global_id(0);
    int b = get_global_id(1);
    if(b>= batch || c >= channel)
        return;
    long index = (long)label[b];
    ulong offset = b*channel + c;
#if REDUCE == 1
    ulong dyoffset = 0;
#else
    ulong dyoffset = b;
#endif    
    dtype dxval = (c == index) ? -scale * dy[dyoffset] : 0;
    if(factor == 0)
        dx[offset] = dxval;
    else
        dx[offset] = dx[offset]*factor + dxval;
}
)kern_src" },{"nll_loss_fwd", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#ifndef CUSTOM_REDUCE
#define CUSTOM_REDUCE 0
#endif

#define my_get_local_wg_id() ((get_local_id(2) * get_local_size(1) * get_local_size(0)) + (get_local_id(1) * get_local_size(0)) + get_local_id(0))
#if __OPENCL_VERSION__ >= 200 && !CUSTOM_REDUCE
#define REDUCE_PREPARE(WG_SIZE,dtype) do {} while(0)
#define my_work_group_reduce_add(val) do { val = work_group_reduce_add(val); } while(0)
#define my_work_group_reduce_max(val) do { val = work_group_reduce_max(val); } while(0)
#else

#define REDUCE_PREPARE(WG_SIZE,dtype) __local dtype my_reduce[WG_SIZE];
#define REDUCE_USING_OP(myval,reduce_op) \
    do { \
        int lid = my_get_local_wg_id(); \
        my_reduce[lid] = myval; \
        barrier(CLK_LOCAL_MEM_FENCE); \
        const int WGS = sizeof(my_reduce)/sizeof(my_reduce[0]); \
        for(int i=WGS / 2;i>0; i>>= 1) { \
            if(lid < i) { \
                my_reduce[lid] = reduce_op(my_reduce[lid],my_reduce[lid+i]); \
            } \
            barrier(CLK_LOCAL_MEM_FENCE); \
        } \
        myval = my_reduce[0]; \
    } while(0)

#define REDUCE_OP_ADD(x,y) ((x) + (y))
#define REDUCE_OP_MAX(x,y) max((x),(y))

#define my_work_group_reduce_add(val) REDUCE_USING_OP(val,REDUCE_OP_ADD)
#define my_work_group_reduce_max(val) REDUCE_USING_OP(val,REDUCE_OP_MAX)

#endif

#ifndef WG_SIZE
#define WG_SIZE 256
#endif

#ifndef ITEMS_PER_WI
#define ITEMS_PER_WI 1
#endif

__kernel 
__attribute__((reqd_work_group_size(WG_SIZE,1,1)))
void nll_loss_forward(int batch,int channels,
             __global dtype const *data,ulong  data_offset,
             __global itype const *label,ulong  label_offset,             
             __global dtype *out,ulong  out_offset,
             dtype scale
             )
{
    data += data_offset;
    label += label_offset;
    out += out_offset;
    
    long item = get_local_id(0) * ITEMS_PER_WI;
    #if REDUCE == 1
    dtype sum = 0;
    #endif

    #pragma unroll
    for(int i=0;i<ITEMS_PER_WI;i++,item++) {
        if(item < batch) {
            long index = (long)(label[item]);
            dtype loss_value = 0;
            if(0<= index && index < channels) {
                loss_value = -data[item*channels + index];
            }
            #if REDUCE==0
            out[item]=loss_value * scale;
            #else
            sum += loss_value;
            #endif
        }
    }
#if REDUCE == 1
    REDUCE_PREPARE(WG_SIZE,dtype);
    my_work_group_reduce_add(sum);
    if(get_local_id(0) == 0) {
        out[0] = sum * scale;
    }
#endif
}


)kern_src" },{"pointwise", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 



__kernel void exec(ulong total  PARAMS)
{
    ulong index=get_global_id(0);
    if(index>=total)
        return;
    LOADS
    CALC
    SAVES
}

)kern_src" },{"pointwise_broadcast", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 



///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
typedef struct Shape {
    ulong s[DIMS];
} Shape;


inline Shape get_pos_broadcast(Shape limits)
{
    Shape r;
#if DIMS <= 1
    r.s[0] = get_global_id(0);
#elif DIMS == 2
    r.s[0] = get_global_id(1);      
    r.s[1] = get_global_id(0);      
#elif DIMS == 3    
    r.s[0] = get_global_id(2);      
    r.s[1] = get_global_id(1);      
    r.s[2] = get_global_id(0);      
#elif DIMS == 4    
    r.s[0] = get_global_id(2);      
    r.s[1] = get_global_id(1);      
    r.s[2] = get_global_id(0) / limits.s[3];      
    r.s[3] = get_global_id(0) % limits.s[3];      
#elif DIMS == 5    
    r.s[0] = get_global_id(2);      
    r.s[1] = get_global_id(1) / limits.s[2];      
    r.s[2] = get_global_id(1) % limits.s[2];      
    r.s[3] = get_global_id(0) / limits.s[4];      
    r.s[4] = get_global_id(0) % limits.s[4];      
#elif DIMS == 6    
    r.s[0] = get_global_id(2) / limits.s[1];      
    r.s[1] = get_global_id(2) % limits.s[1];      
    r.s[2] = get_global_id(1) / limits.s[3];      
    r.s[3] = get_global_id(1) % limits.s[3];      
    r.s[4] = get_global_id(0) / limits.s[5];      
    r.s[5] = get_global_id(0) % limits.s[5];      
#elif DIMS == 7
    r.s[0] = get_global_id(2) / limits.s[1];      
    r.s[1] = get_global_id(2) % limits.s[1];      
    r.s[2] = get_global_id(1) / limits.s[3];      
    r.s[3] = get_global_id(1) % limits.s[3];      
    r.s[4] = get_global_id(0) / (limits.s[5]*limits.s[6]);      
    ulong s56 = get_global_id(0) % (limits.s[5]*limits.s[6]);
    r.s[5] = s56 / limits.s[6];      
    r.s[6] = s56 % limits.s[6];      
#elif DIMS == 8
    r.s[0] = get_global_id(2) / limits.s[1];      
    r.s[1] = get_global_id(2) % limits.s[1];      

    r.s[2] = get_global_id(1) / (limits.s[3]*limits.s[4]);      
    ulong s34 = get_global_id(1) % (limits.s[3]*limits.s[4]);
    r.s[3] = s34 / limits.s[4];      
    r.s[4] = s34 % limits.s[4];      

    r.s[5] = get_global_id(0) / (limits.s[6]*limits.s[7]);      
    ulong s67 = get_global_id(0) % (limits.s[6]*limits.s[7]);
    r.s[6] = s67 / limits.s[7];      
    r.s[7] = s67 % limits.s[7];      
#else
#error "Unsupported dim"
#endif
    return r;
}


inline ulong get_offset(Shape s,Shape strides,ulong offset)
{
    ulong r = offset;
    #pragma unroll
    for(int i=0;i<DIMS;i++) {
        r+= s.s[i]*strides.s[i];
    }
    return r;
}

inline ulong get_direct_offset(Shape s,Shape sizes,ulong offset)
{
    ulong index = 0;
    #pragma unroll
    for(int i=0;i<DIMS-1;i++) {
        index += s.s[i];
        index *= sizes.s[i+1];
    }
    index += s.s[DIMS-1] + offset;
    return index;
}

#define get_pos(limits) get_pos_broadcast(limits)

inline bool valid_pos(Shape pos,Shape limits)
{
    #pragma unroll
    for(int i=0;i<DIMS;i++)
        if(pos.s[i] >= limits.s[i])
            return 0;
    return 1;

}

__kernel void exec(Shape limit   PARAMS)
{
    Shape index = get_pos(limit);
    if(!valid_pos(index,limit)) {
        return;
    }
    LOADS
    CALC
    SAVES
}


)kern_src" },{"pointwise_broadcast_reduce", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#ifndef REDUCE_DIMS
#error "REDUCE_DIMS must be defined"
#endif
#ifndef DIMS
#error "DIMS must be defined"
#endif

///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
typedef struct Shape {
    ulong s[DIMS];
} Shape;


inline Shape get_pos_broadcast(Shape limits)
{
    Shape r;
#if DIMS <= 1
    r.s[0] = get_global_id(0);
#elif DIMS == 2
    r.s[0] = get_global_id(1);      
    r.s[1] = get_global_id(0);      
#elif DIMS == 3    
    r.s[0] = get_global_id(2);      
    r.s[1] = get_global_id(1);      
    r.s[2] = get_global_id(0);      
#elif DIMS == 4    
    r.s[0] = get_global_id(2);      
    r.s[1] = get_global_id(1);      
    r.s[2] = get_global_id(0) / limits.s[3];      
    r.s[3] = get_global_id(0) % limits.s[3];      
#elif DIMS == 5    
    r.s[0] = get_global_id(2);      
    r.s[1] = get_global_id(1) / limits.s[2];      
    r.s[2] = get_global_id(1) % limits.s[2];      
    r.s[3] = get_global_id(0) / limits.s[4];      
    r.s[4] = get_global_id(0) % limits.s[4];      
#elif DIMS == 6    
    r.s[0] = get_global_id(2) / limits.s[1];      
    r.s[1] = get_global_id(2) % limits.s[1];      
    r.s[2] = get_global_id(1) / limits.s[3];      
    r.s[3] = get_global_id(1) % limits.s[3];      
    r.s[4] = get_global_id(0) / limits.s[5];      
    r.s[5] = get_global_id(0) % limits.s[5];      
#elif DIMS == 7
    r.s[0] = get_global_id(2) / limits.s[1];      
    r.s[1] = get_global_id(2) % limits.s[1];      
    r.s[2] = get_global_id(1) / limits.s[3];      
    r.s[3] = get_global_id(1) % limits.s[3];      
    r.s[4] = get_global_id(0) / (limits.s[5]*limits.s[6]);      
    ulong s56 = get_global_id(0) % (limits.s[5]*limits.s[6]);
    r.s[5] = s56 / limits.s[6];      
    r.s[6] = s56 % limits.s[6];      
#elif DIMS == 8
    r.s[0] = get_global_id(2) / limits.s[1];      
    r.s[1] = get_global_id(2) % limits.s[1];      

    r.s[2] = get_global_id(1) / (limits.s[3]*limits.s[4]);      
    ulong s34 = get_global_id(1) % (limits.s[3]*limits.s[4]);
    r.s[3] = s34 / limits.s[4];      
    r.s[4] = s34 % limits.s[4];      

    r.s[5] = get_global_id(0) / (limits.s[6]*limits.s[7]);      
    ulong s67 = get_global_id(0) % (limits.s[6]*limits.s[7]);
    r.s[6] = s67 / limits.s[7];      
    r.s[7] = s67 % limits.s[7];      
#else
#error "Unsupported dim"
#endif
    return r;
}


#define NORMAL_DIMS (DIMS - REDUCE_DIMS)

#if REDUCE_DIMS > DIMS
#error "REDUCE_DIMS must be <= DIMS"
#endif
#if REDUCE_DIMS < 0
#error "Need at least 1 dim for reduction"
#endif

#ifndef SMALL_REDUCTION
#define SMALL_REDUCTION 0
#endif

#ifndef TWO_STAGE_REDUCTION
#define TWO_STAGE_REDUCTION 0
#endif


inline ulong get_base_offset(Shape s,Shape strides,ulong offset)
{
    ulong r = offset;
    #pragma unroll
    for(int i=REDUCE_DIMS;i<DIMS;i++) {
        r+= s.s[i]*strides.s[i];
    }
    return r;
}

inline ulong get_reduce_offset(Shape s,Shape strides)
{
    ulong r = 0;
    #pragma unroll
    for(int i=0;i<REDUCE_DIMS;i++) {
        r+= s.s[i]*strides.s[i];
    }
    return r;
}


void next_pos(Shape limits,Shape *pos)
{
#if REDUCE_DIMS == 0
    /// nothing
#elif REDUCE_DIMS == 1
    pos->s[0] ++;
#elif REDUCE_DIMS == 2
    pos->s[1]++;
    if(pos->s[1] == limits.s[1]) {
        pos->s[1] = 0;
        pos->s[0] ++;
    }
#elif REDUCE_DIMS == 3
    pos->s[2]++;
    if(pos->s[2] == limits.s[2]) {
        pos->s[2] = 0;
        pos->s[1] ++;
        if(pos->s[1] == limits.s[1]) {
            pos->s[1] = 0;
            pos->s[0] ++;
        }
    }
#else 
// for total dims limit = 5 shouldn't be more than 3 reduction dims otherwise they will be shrinked
#error "Too many reduction dims"
#endif

}

#if REDUCE_DIMS >= 1
inline Shape get_pos(Shape limits,ulong reduce_item)
{
    Shape r;
#if REDUCE_DIMS == 1
    r.s[0] = reduce_item;
#elif REDUCE_DIMS == 2
    r.s[0] = reduce_item / limits.s[1];
    r.s[1] = reduce_item % limits.s[1];
#elif REDUCE_DIMS == 3
    r.s[2] = reduce_item % limits.s[2];
    ulong ri2 = reduce_item / limits.s[2];
    r.s[1] = ri2 % limits.s[1];
    r.s[0] = ri2 / limits.s[1];
#else 
// for total dims limit = 5 shouldn't be more than 3 reduction dims otherwise they will be shrinked
#error "Too many reduction dims"
#endif

#if NORMAL_DIMS == 0
    // nothing
#elif NORMAL_DIMS == 1
    r.s[REDUCE_DIMS + 0] = get_global_id(1);
#elif NORMAL_DIMS == 2
    r.s[REDUCE_DIMS + 0] = get_global_id(2);      
    r.s[REDUCE_DIMS + 1] = get_global_id(1);      
#elif NORMAL_DIMS == 3
    r.s[REDUCE_DIMS + 0] = get_global_id(2) / limits.s[REDUCE_DIMS+1];      
    r.s[REDUCE_DIMS + 1] = get_global_id(2) % limits.s[REDUCE_DIMS+1];      
    r.s[REDUCE_DIMS + 2] = get_global_id(1);      
#elif NORMAL_DIMS == 4
    r.s[REDUCE_DIMS + 0] = get_global_id(2) / limits.s[REDUCE_DIMS+1];      
    r.s[REDUCE_DIMS + 1] = get_global_id(2) % limits.s[REDUCE_DIMS+1];      
    r.s[REDUCE_DIMS + 2] = get_global_id(1) / limits.s[REDUCE_DIMS+3];
    r.s[REDUCE_DIMS + 3] = get_global_id(1) % limits.s[REDUCE_DIMS+3];
#else
#error "Unsupported dim"
#endif
    return r;
}

#endif

inline bool valid_save_pos(Shape pos,Shape limits)
{
    #pragma unroll
    for(int i=REDUCE_DIMS;i<DIMS;i++)
        if(pos.s[i] >= limits.s[i])
            return 0;
    return 1;

}


inline bool valid_pos(Shape pos,Shape limits)
{
    #pragma unroll
    for(int i=0;i<DIMS;i++)
        if(pos.s[i] >= limits.s[i])
            return 0;
    return 1;

}

#define PARAM_INPUT(type,I) ,__global type const *px##I,ulong px##I##_offset,Shape xstrides##I
#if TWO_STAGE_REDUCTION == 1
#define PARAM_OUTPUT(type,ptype,I) ,__global type *py##I,ulong py##I##_offset,Shape ystrides##I
#else
#define PARAM_OUTPUT(type,ptype,I) ,__global type *py##I,ulong py##I##_offset,Shape ystrides##I,ptype alpha##I,ptype beta##I
#endif
#define PAPAM_WEIGHT(type,I) ,type w##I


#define PREPARE_LOAD_INPUT(type,I) \
    ulong input_offset_##I = get_base_offset(index,xstrides##I,px##I##_offset); \
    type x##I;

#define LOAD_INPUT(I) x##I = px##I[input_offset_##I + get_reduce_offset(index,xstrides##I)];
#define SAVE_OUTPUT(I) py##I[get_base_offset(index,ystrides##I,py##I##_offset)] = reduce_y##I;

#define my_get_local_wg_id() ((get_local_id(2) * get_local_size(1) * get_local_size(0)) + (get_local_id(1) * get_local_size(0)) + get_local_id(0))

#if SMALL_REDUCTION == 1
#define REDUCE_INIT(type,I) type reduce_y##I,y##I;
#else
#define REDUCE_INIT(type,I) \
    __local type my_reduce_##I[WG_SIZE]; \
    type reduce_y##I,y##I; 
#endif    

#define SAVE_REDUCE(I) my_reduce_##I[lid] = reduce_y##I;
#define LOAD_REDUCE(I) reduce_y##I = my_reduce_##I[lid]; y##I = my_reduce_##I[nxt];

#if SMALL_REDUCTION == 1
#define LOAD_REDUCED_SAVE_GLOBAL(I) \
do { \
    py##I += get_base_offset(index,ystrides##I,py##I##_offset); \
    reduce_y##I *= alpha##I; \
    if(beta##I)  \
        *py##I = beta##I * *py##I + reduce_y##I; \
    else \
        *py##I = reduce_y##I; \
}while(0)
#elif TWO_STAGE_REDUCTION == 0
#define LOAD_REDUCED_SAVE_GLOBAL(I) \
do { \
    y##I = alpha##I * my_reduce_##I[0]; \
    py##I += get_base_offset(index,ystrides##I,py##I##_offset); \
    if(beta##I) \
        *py##I = beta##I * *py##I + y##I; \
    else \
        *py##I = y##I; \
} while(0)
#else //TWO_STAGE_REDUCTION == 1
#define LOAD_REDUCED_SAVE_GLOBAL(I) \
do { \
    py##I += py##I##_offset + get_group_id(0); \
    py##I += reduce_stride * get_base_offset(index,ystrides##I,0); \
    *py##I = my_reduce_##I[0]; \
} while(0)
#endif


__kernel 
#if SMALL_REDUCTION == 0
__attribute__((reqd_work_group_size(WG_SIZE,1,1)))
#endif
void exec(Shape limit 
                   PARAMS
#if TWO_STAGE_REDUCTION == 1
                   ,ulong reduce_stride
#endif                                      
                   )

{
#if REDUCE_DIMS == 0
    #if ITEMS_PER_WI > 1
    #error "Invalid Items per wi size"
    #endif
    ulong reduce_item = 0;
    Shape index0 = get_pos_broadcast(limit);
#else    
    ulong reduce_item = get_global_id(0) * ITEMS_PER_WI;
    Shape index0 = get_pos(limit,reduce_item);
#endif
    Shape index = index0;
    PREPARE_LOAD_INPUT_ALL
    REDUCE_INIT_ALL

    #pragma unroll(8)
    for(int item=0;item < ITEMS_PER_WI;item++) {
        if(valid_pos(index,limit)) {
            LOAD_INPUT_ALL
            CALC
            REDUCE
        }
#if ITEMS_PER_WI > 1
        next_pos(limit,&index);
        reduce_item ++;
#endif
    }

    #if SMALL_REDUCTION == 0

    int lid = get_local_id(0); 

    SAVE_REDUCE_ALL
    
    barrier(CLK_LOCAL_MEM_FENCE); 
    for(int i= WG_SIZE / 2;i>0; i>>= 1) { 
        if(lid < i) { 
            int nxt = lid+i;
            LOAD_REDUCE_ALL
            REDUCE
            SAVE_REDUCE_ALL
        } 
        barrier(CLK_LOCAL_MEM_FENCE); 
    } 
    if(lid == 0) {
        if(valid_save_pos(index0,limit)) {
            LOAD_REDUCED_SAVE_GLOBAL_ALL
        }
    }

    #else
    if(valid_save_pos(index0,limit)) {
        LOAD_REDUCED_SAVE_GLOBAL_ALL
    }
    #endif
}


)kern_src" },{"pooling", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
void atomic_addf(__global volatile float *ptr,float v)
{
#if defined(__opencl_c_ext_fp32_global_atomic_add)
    atomic_fetch_add((__global volatile atomic_float *)ptr,v);
#elif defined(cl_intel_subgroups)
    __global atomic_int *p = (__global atomic_int *)(ptr);
    int prev,newv;
    do {
        prev = atomic_load(p);
        newv = as_int(as_float(prev) + v);
    } while(! atomic_compare_exchange_weak(p,&prev,newv));
#elif defined(__NV_CL_C_VERSION)
    float prev;
    asm volatile(
        "atom.global.add.f32 %0, [%1], %2;"
        : "=f"(prev)
        : "l"(ptr) , "f"(v)
        : "memory"
    );
#else
    float oldv = *ptr;
    for(;;) {
        float newv = oldv + v;
        int prev = atomic_cmpxchg((__global volatile int *)(ptr),as_int(oldv),as_int(newv));
        if(prev == as_int(oldv))
            return;
        oldv = as_float(prev);
    }
#endif    
}


#ifndef itype
#define itype int
#endif

#ifndef POOL_MODE
#define POOL_MODE 0
#endif

#ifndef POOL_H
#define POOL_H 1
#endif
#ifndef POOL_W
#define POOL_W 1
#endif

#ifndef STRIDE_H 
#define STRIDE_H 1
#endif

#ifndef STRIDE_W
#define STRIDE_W 1
#endif

#ifndef PAD_H 
#define PAD_H 0
#endif

#ifndef PAD_W
#define PAD_W 0
#endif

#ifndef COUNT_INCLUDE_PAD
#define COUNT_INCLUDE_PAD 0
#endif

#if POOL_MODE == 0
# define START_VAL -DTYPE_MAX
# define REDUCE(a,b) max((a),(b))
# define NORMALIZE_FULL(x) (x)
# define NORMALIZE_PARTIAL(x,dr,dc,vdr,vdc) (x)
#elif POOL_MODE == 1
# define START_VAL 0.0f
# define REDUCE(a,b) ((a) + (b))
# define NORMALIZE_FULL(x) ((x) * (1.0f / (POOL_H * POOL_W)))
# if COUNT_INCLUDE_PAD == 0
#  define NORMALIZE_PARTIAL(x,dr,dc,vdr,vdc) ((x) * (1.0f /((dr)*(dc))))
# else
#  define NORMALIZE_PARTIAL(x,dr,dc,vdr,vdc) ((x) * (1.0f /((vdr)*(vdc))))
# endif
#else
#error "Invalid mode"
#endif

#ifndef WG_SIZE
#define WG_SIZE 8
#endif

#if POOL_MODE == 0 && EXPORT_INDEX == 1
#define INDEX_MAX_SRC 1
#else
#define INDEX_MAX_SRC 0
#endif


__kernel
__attribute__((reqd_work_group_size(WG_SIZE,WG_SIZE,1)))
void pooling(int BC,int inp_H,int inp_W,int out_H,int out_W,
             __global const dtype *src,ulong src_offset,
             __global dtype *tgt,ulong tgt_offset
#if INDEX_MAX_SRC == 1
             ,__global itype *indx,ulong indx_offset
#endif                                       
             
             )
{
    int out_r = get_global_id(0);
    int out_c = get_global_id(1);
    int bc = get_global_id(2);
    if(bc >= BC || out_r >= out_H || out_c >= out_W)
        return;

    int row0 = out_r * STRIDE_H - PAD_H;
    int col0 = out_c * STRIDE_W - PAD_W;
    int row1 = row0 + POOL_H;
    int col1 = col0 + POOL_W;

    tgt += tgt_offset + bc * out_H * out_W;
    src += src_offset + bc * inp_H * inp_W;

    dtype val = START_VAL;
    #if INDEX_MAX_SRC == 1
    itype index = -1;
    indx += indx_offset + bc * out_H * out_W;
    #endif
    
    if(row0 >= 0 && col0 >= 0 && row1 <= inp_H && col1 <= inp_W) {
        src += row0 * inp_W + col0;
        #pragma unroll  
        for(int dr=0;dr<POOL_H;dr++) {
            #pragma unroll
            for(int dc = 0;dc < POOL_W; dc++) {
                #if INDEX_MAX_SRC == 1
                dtype tmp = src[dr * inp_W + dc];
                if(tmp > val) {
                    index = (row0 + dr) * inp_W + col0 + dc;
                    val = tmp;
                }
                #else
                val = REDUCE(val,src[dr * inp_W + dc]);
                #endif
            }
        }
        val = NORMALIZE_FULL(val); 
    }
    else {
        #pragma unroll
        for(int r=row0;r<row1;r++) {
            #pragma unroll
            for(int c=col0;c<col1;c++) {
                dtype loaded_val = (r >= 0 && r<inp_H && c>=0 && c<inp_W) ? src[r*inp_W + c] : START_VAL;
                #if INDEX_MAX_SRC == 1
                if(loaded_val > val) {
                    index = r*inp_W + c;
                    val = loaded_val;
                }
                #else
                val = REDUCE(val,loaded_val);
                #endif
            }
        }
        val = NORMALIZE_PARTIAL(val, min(row1,inp_H) - max(row0,0),
                                     min(col1,inp_W) - max(col0,0),
                                     min(row1,inp_H + PAD_H) - max(-PAD_H,row0),
                                     min(col1,inp_W + PAD_W) - max(-PAD_W,col0)
                                     );
    }
    tgt[out_r * out_W + out_c] = val;
    #if INDEX_MAX_SRC == 1
    indx[out_r * out_W + out_c] = index;
    #endif
}

void save_dx(__global dtype *ptr,dtype value)
{
    #if POOL_W <= STRIDE_W && POOL_H <= STRIDE_H
    *ptr = value + *ptr;
    #else
    atomic_addf(ptr,value);
    #endif

}

#if INDEX_MAX_SRC == 1
__kernel
__attribute__((reqd_work_group_size(WG_SIZE,WG_SIZE,1)))
void pooling_bw(int BC,int inp_H,int inp_W,int out_H,int out_W,
             __global dtype *src,ulong src_offset,
             __global const dtype *tgt,ulong tgt_offset,
             __global const itype *indx,ulong indx_offset)
{
    int out_r = get_global_id(0);
    int out_c = get_global_id(1);
    int bc = get_global_id(2);

    if(bc >= BC || out_r >= out_H || out_c >= out_W)
        return;

    int row0 = out_r * STRIDE_H - PAD_H;
    int col0 = out_c * STRIDE_W - PAD_W;
    int row1 = row0 + POOL_H;
    int col1 = col0 + POOL_W;

    tgt  += tgt_offset + bc * out_H * out_W;
    src  += src_offset + bc * inp_H * inp_W;
    indx += indx_offset + bc * out_H * out_W;

    dtype dy  =  tgt[out_r * out_W + out_c];
    itype pos = indx[out_r * out_W + out_c];

    save_dx(src+pos,dy);
}

#elif POOL_MODE == 0 // max pooling
__kernel
__attribute__((reqd_work_group_size(WG_SIZE,WG_SIZE,1)))
void pooling_bw(int BC,int inp_H,int inp_W,int out_H,int out_W,
             __global const dtype *src,ulong src_offset,
             __global const dtype *tgt,ulong tgt_offset,
             __global dtype *dx,ulong dx_offset)
{
    int out_r = get_global_id(0);
    int out_c = get_global_id(1);
    int bc = get_global_id(2);

    if(bc >= BC || out_r >= out_H || out_c >= out_W)
        return;

    int row0 = out_r * STRIDE_H - PAD_H;
    int col0 = out_c * STRIDE_W - PAD_W;
    int row1 = row0 + POOL_H;
    int col1 = col0 + POOL_W;

    tgt += tgt_offset + bc * out_H * out_W;
    src += src_offset + bc * inp_H * inp_W;
    dx  += dx_offset  + bc * inp_H * inp_W;

    dtype val = START_VAL;
    itype index = -1;
    
    if(row0 >= 0 && col0 >= 0 && row1 <= inp_H && col1 <= inp_W) {
        src += row0 * inp_W + col0;
        #pragma unroll  
        for(int dr=0;dr<POOL_H;dr++) {
            #pragma unroll
            for(int dc = 0;dc < POOL_W; dc++) {
                dtype tmp = src[dr * inp_W + dc];
                if(tmp > val) {
                    index = (row0 + dr) * inp_W + col0 + dc;
                    val = tmp;
                }
            }
        }
    }
    else {
        #pragma unroll
        for(int r=row0;r<row1;r++) {
            #pragma unroll
            for(int c=col0;c<col1;c++) {
                dtype loaded_val = (r >= 0 && r<inp_H && c>=0 && c<inp_W) ? src[r*inp_W + c] : START_VAL;
                if(loaded_val > val) {
                    index = r*inp_W + c;
                    val = loaded_val;
                }
            }
        }
    }
    
    dtype dy = tgt[out_r * out_W + out_c];

    save_dx(dx+index,dy);
}

#elif POOL_MODE == 1
__kernel
__attribute__((reqd_work_group_size(WG_SIZE,WG_SIZE,1)))
void pooling_bw(int BC,int inp_H,int inp_W,int out_H,int out_W,
             __global const dtype *tgt,ulong tgt_offset,
             __global dtype *dx,ulong dx_offset)
{
    int out_r = get_global_id(0);
    int out_c = get_global_id(1);
    int bc = get_global_id(2);
    if(bc >= BC || out_r >= out_H || out_c >= out_W)
        return;

    int row0 = out_r * STRIDE_H - PAD_H;
    int col0 = out_c * STRIDE_W - PAD_W;
    int row1 = row0 + POOL_H;
    int col1 = col0 + POOL_W;

    tgt += tgt_offset + bc * out_H * out_W;
    dx  += dx_offset  + bc * inp_H * inp_W;

    dtype dy = tgt[out_r * out_W + out_c];
    if(row0 >= 0 && col0 >= 0 && row1 <= inp_H && col1 <= inp_W) {
        dtype dy_norm = NORMALIZE_FULL(dy);
        dx += row0 * inp_W + col0;
        #pragma unroll  
        for(int dr=0;dr<POOL_H;dr++) {
            #pragma unroll
            for(int dc = 0;dc < POOL_W; dc++) {
                save_dx(dx + dr * inp_W + dc,dy_norm);
            }
        }
    }
    else {
        dtype dy_norm = NORMALIZE_PARTIAL(dy, min(row1,inp_H)-max(row0,0),
                                              min(col1,inp_W) - max(col0,0),
                                              min(row1,inp_H + PAD_H) - max(-PAD_H,row0),
                                              min(col1,inp_W + PAD_W) - max(-PAD_W,col0)
                                            );
        #pragma unroll
        for(int r=row0;r<row1;r++) {
            #pragma unroll
            for(int c=col0;c<col1;c++) {
                if(r >= 0 && r<inp_H && c>=0 && c<inp_W)
                    save_dx(dx + r*inp_W + c,dy_norm);
            }
        }
    }
}
#else
#error "Invalid mode"
#endif

)kern_src" },{"random", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
inline uint mulhi(uint a,uint b)
{
    ulong v=a;
    v*=b;
    return v>>32;
}

inline uint mullo(uint a,uint b)
{
    ulong v=a;
    v*=b;
    return v;
}


typedef struct state {
    uint l0,r0,l1,r1;
    uint k0,k1;
} state;

inline state single_round(state s)
{
    state next;
    next.l1 = mullo(s.r1, 0xD2511F53);
    next.r1 = mulhi(s.r0, 0xCD9E8D57) ^ s.k0 ^ s.l0;
    next.l0 = mullo(s.r0, 0xCD9E8D57);
    next.r0 = mullo(s.r1, 0xD2511F53) ^ s.k1 ^ s.l1;
    next.k0 = s.k0 + 0xBB67AE85;
    next.k1 = s.k1 + 0x9E3779B9;
    return next;
}

state make_initial_state(ulong seed,ulong sequence)
{
    state s;
    s.l1 = sequence >> 32;
    s.r1 = sequence;
    s.l0 = 0;
    s.r0 = 0;
    s.k0 = seed;
    s.k1 = seed >> 32;
    return s;
}

inline uint4 calculate(state s)
{
    #pragma unroll
    for(int i=0;i<10;i++)
        s=single_round(s);
    uint4 r;
    r.s0 = s.l0;
    r.s1 = s.r0;
    r.s2 = s.l1;
    r.s3 = s.r1;
    return r;
}

float4 calculate_float(state s)
{
    uint4 r = calculate(s);
    float4 f;
    /// make sure float does not become 1 after rounding
    /// 24 - for float/bfloat16
    /// 16 - for half
    /// 32 - for double
    const int accuracy_shift = 24;
    const int drop_bits = 32 - accuracy_shift;
    const float factor = 1.0f / ((ulong)(1) << accuracy_shift);
    f.s0 = (r.s0 >> drop_bits) * factor;
    f.s1 = (r.s1 >> drop_bits) * factor;
    f.s2 = (r.s2 >> drop_bits) * factor;
    f.s3 = (r.s3 >> drop_bits) * factor;
    return f;
}

#if IS_NORMAL == 1
float2 normal_pair(float2 v)
{
    float scale = sqrt(-2.0f*log(1.0f - v.s0));
    float angle = (2.0f*3.1415926535f)*v.s1;
    return (float2)(scale*cos(angle),scale*sin(angle));
}
#endif

__kernel void fill(ulong total,__global float *p,ulong p_offset,ulong seed,ulong seq,float v1,float v2)
{
    ulong pos = get_global_id(0);
    if(pos * 4 >= total)
        return;
    p+=p_offset;
    seq += pos;
    state s = make_initial_state(seed,seq);
    float4 r = calculate_float(s);
#if IS_UNIFORM == 1
    r = r * (float4)(v2-v1) + (float4)(v1);
#endif
#if IS_BERNOULLI == 1
    r.s0 = r.s0 < v1 ? 1:0;
    r.s1 = r.s1 < v1 ? 1:0;
    r.s2 = r.s2 < v1 ? 1:0;
    r.s3 = r.s3 < v1 ? 1:0;
#endif    
#if IS_NORMAL == 1
    r.lo = normal_pair(r.lo);
    r.hi = normal_pair(r.hi);
    r = r*(float4)(v2) + (float4)(v1);
#endif    
    ulong index = pos * 4;
    if(index < total) {
        vstore4(r,0,p + index);
    }
    else {
        if(index + 0 < total) p[index + 0]=r.s0;
        if(index + 1 < total) p[index + 1]=r.s1;
        if(index + 2 < total) p[index + 2]=r.s2;
        if(index + 3 < total) p[index + 3]=r.s3;
    }
}
)kern_src" },{"scal", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
__kernel
void sscal(ulong size,float scale,__global float *p,ulong p_off)
{
    ulong pos = get_global_id(0);
    if(pos >= size)
        return;
    p+=p_off;
    if(scale == 0)
        p[pos] = 0;
    else
        p[pos] = p[pos] * scale;
}
)kern_src" },{"sgemm", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
// vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


#ifndef TILE_SIZE_M
#define TILE_SIZE_M 128
#endif
#ifndef TILE_SIZE_N
#define TILE_SIZE_N 128
#endif
#ifndef BLOCK_SIZE_N
#define BLOCK_SIZE_N 8
#endif
#ifndef BLOCK_SIZE_M
#define BLOCK_SIZE_M 8
#endif

#ifndef TILE_SIZE_K
#define TILE_SIZE_K 16
#endif 

#ifndef TILE_OFFSET
#define TILE_OFFSET 1
#endif

#ifndef ZORDER
#define ZORDER 0
#endif


#ifndef ATRANS
#define ATRANS 0
#endif

#ifndef BTRANS
#define BTRANS 0
#endif

#ifndef BIAS
#define BIAS 0
#endif

#ifndef GROUPS
#define GROUPS 1
#endif

#define BLOCK_SIZE_NY (BLOCK_SIZE_N*BLOCK_SIZE_M)
#define BLOCKS_IN_TILE_N (TILE_SIZE_N / BLOCK_SIZE_N)
#define BLOCKS_IN_TILE_M (TILE_SIZE_M / BLOCK_SIZE_M)
#define WG_SIZE (BLOCKS_IN_TILE_M * BLOCKS_IN_TILE_N)


#define ALIGN_FLOAT4 __attribute__ ((aligned (16)))

#ifndef CONVGEMM
#define CONVGEMM 0
#endif

#if CONVGEMM == 3 || REDUCE_K > 1
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
void atomic_addf(__global volatile float *ptr,float v)
{
#if defined(__opencl_c_ext_fp32_global_atomic_add)
    atomic_fetch_add((__global volatile atomic_float *)ptr,v);
#elif defined(cl_intel_subgroups)
    __global atomic_int *p = (__global atomic_int *)(ptr);
    int prev,newv;
    do {
        prev = atomic_load(p);
        newv = as_int(as_float(prev) + v);
    } while(! atomic_compare_exchange_weak(p,&prev,newv));
#elif defined(__NV_CL_C_VERSION)
    float prev;
    asm volatile(
        "atom.global.add.f32 %0, [%1], %2;"
        : "=f"(prev)
        : "l"(ptr) , "f"(v)
        : "memory"
    );
#else
    float oldv = *ptr;
    for(;;) {
        float newv = oldv + v;
        int prev = atomic_cmpxchg((__global volatile int *)(ptr),as_int(oldv),as_int(newv));
        if(prev == as_int(oldv))
            return;
        oldv = as_float(prev);
    }
#endif    
}

#if ACTIVATION != ACTIVATION_IDENTITY
# error "Can't use activation with atomic ops"
#endif
#endif

#if CONVGEMM != 0
    #if CONVGEMM == 3
    void add_img_value(__global float *ptr,int matrix_row,int matrix_col,float dV)
    #else
    float get_img_value(__global float const *ptr,int matrix_row,int matrix_col)
    #endif
    {
#if KERN_W == 1 && KERN_H == 1 && STRIDE_H == 1 && STRIDE_W == 1 && DILATE_H == 1 && DILATE_W == 1 \
        && PAD_H == 0 && PAD_W == 0 && GROUPS == 1 && IMG_COLS == SRC_COLS && IMG_ROWS == SRC_ROWS 
        int channel = matrix_col;
        int b  = matrix_row / (IMG_COLS * IMG_ROWS);
        int rc = matrix_row % (IMG_COLS * IMG_ROWS);

        int address = (b * CHANNELS_IN + channel) * (SRC_ROWS * SRC_COLS) + rc;
        #if CONVGEMM != 3
            return ptr[address];
        #else
            #if REDUCE_K > 1
                atomic_addf(ptr+address,dV);
            #else
                ptr[address] += dV;
            #endif
        #endif
#else
        int channel = matrix_col / (KERN_H * KERN_W);
        int k_index = matrix_col % (KERN_H * KERN_W);
        
        int dy = k_index / KERN_W;
        int dx = k_index % KERN_W;
        
        int b  = matrix_row / (IMG_COLS * IMG_ROWS);
        int rc = matrix_row % (IMG_COLS * IMG_ROWS);

        int r  = rc / IMG_COLS;
        int c  = rc % IMG_COLS;

        int y_pos = -PAD_H + r * STRIDE_H;
        int x_pos = -PAD_W + c * STRIDE_W;

        int y = y_pos + dy * DILATE_H;
        int x = x_pos + dx * DILATE_W;

        int address = ((b *  (CHANNELS_IN*GROUPS) + channel) * SRC_ROWS + y) * SRC_COLS + x;

        #if CONVGEMM != 3
            #if PAD_W > 0 || PAD_H > 0
                if(x >= 0 && y >= 0 && x < SRC_COLS && y < SRC_ROWS) {
                    return ptr[address];
                }
                return 0;
            #else
                return ptr[address];
            #endif  
        #else
            #if PAD_W > 0 || PAD_H > 0
                if(x >= 0 && y >= 0 && x < SRC_COLS && y < SRC_ROWS) 
                    atomic_addf(ptr+address,dV);
            #else
                atomic_addf(ptr+address,dV);
            #endif
        #endif
#endif  
    }
#endif



#if CONVGEMM == 0 || CONVGEMM == 3
#  if BTRANS == 0
#    define get_B(r,c) (B[(r)*ldb + (c)])
#  else
#    define get_B(r,c) (B[(c)*ldb + (r)])
#  endif
#else
#  if BTRANS == 0
#    define get_B(r,c) get_img_value(B,r,c)
#  else
#    define get_B(r,c) get_img_value(B,c,r)
#  endif
#endif

#if CONVGEMM  == 0 || CONVGEMM == 1
    #if  ATRANS == 0
        #define get_A(r,c) (A[(r)*lda + (c)])
    #else
        #define get_A(r,c) (A[(c)*lda + (r)])
    #endif
#else
    float get_y_value(int row,int matrix_col,__global float const *A,int ldc,int M)
    {
        int batch = matrix_col / IM2COL_OCHAN;
        int incol = matrix_col % IM2COL_OCHAN;
        int offset = batch * (IM2COL_OCHAN * GROUPS) * M + incol;
        int index =row*IM2COL_OCHAN + offset;
        return A[index];
    }

    #if CONVGEMM == 3
        #define GET_Y_STEP K_src
    #else
        #define GET_Y_STEP M
    #endif
    #if  ATRANS == 0
        #define get_A(r,c) (get_y_value(r,c,A,lda,GET_Y_STEP))
    #else
        #define get_A(r,c) (get_y_value(c,r,A,lda,GET_Y_STEP))
    #endif

#endif

#define lA(x,y) a_tile[(x)][(y) / BLOCK_SIZE_M][(y) % BLOCK_SIZE_M]
#define lB(x,y) b_tile[(x)][(y) / BLOCK_SIZE_N][(y) % BLOCK_SIZE_N]


#if TILE_SIZE_M != TILE_SIZE_N
#error "Unsupported condif"
#endif

#if defined(cl_intel_subgroups)
#define INTEL_PLATFORM 1
#else
#define INTEL_PLATFORM 0
#endif

#define vload1(off,addr) ((addr)[off])
#define vstore1(val,off,addr) ((addr)[off]=(val))

#if BLOCK_SIZE_M == 1
#define vloadM vload1
#define vstoreM vstore1
#define floatM float
#elif BLOCK_SIZE_M == 4
#define vloadM vload4
#define vstoreM vstore4
#define floatM float4
#elif BLOCK_SIZE_M == 8
#define vloadM vload8
#define vstoreM vstore8
#define floatM float8
#elif BLOCK_SIZE_M == 16 
#define vloadM vload16
#define vstoreM vstore16
#define floatM float16
#endif

#if BLOCK_SIZE_N == 1
#define vloadN vload1
#define vstoreN vstore1
#define floatN float
#elif BLOCK_SIZE_N == 4
#define vloadN vload4
#define vstoreN vstore4
#define floatN float4
#elif BLOCK_SIZE_N == 8
#define vloadN vload8
#define vstoreN vstore8
#define floatN float8
#elif BLOCK_SIZE_N == 16 
#define vloadN vload16
#define vstoreN vstore16
#define floatN float16
#endif

#if TILE_SIZE_K == 1
#define vloadK vload1
#define vstoreK vstore1
#define floatK float
#elif TILE_SIZE_K == 4
#define vloadK vload4
#define vstoreK vstore4
#define floatK float4
#elif TILE_SIZE_K == 8
#define floatK float8
#define vloadK vload8
#define vstoreK vstore8
#elif TILE_SIZE_K == 16 
#define vloadK vload16
#define vstoreK vstore16
#define floatK float16
#endif


#ifndef BATCH_GEMM
#define BATCH_GEMM 0
#endif

#ifndef REDUCE_K
#define REDUCE_K 1
#endif

#if GROUPS == 1 && REDUCE_K == 1 && BATCH_GEMM == 0
#define DIM_M 0
#define DIM_N 1
#define DIM_G 2
#define EXTRA_DIM 0
#else
#define DIM_M 1
#define DIM_N 2
#define DIM_G 0
#define EXTRA_DIM 1
#endif

int zorder_a(int x)
{
    return
          ((x & (1<<0)) >> 0 )
        | ((x & (1<<2)) >> 1 )
        | ((x & (1<<4)) >> 2 )
        | ((x & (1<<6)) >> 3 )
        | ((x & (1<<8)) >> 4 )
        | ((x & (1<<10)) >> 5 )
        | ((x & (1<<12)) >> 6 )
        | ((x & (1<<14)) >> 7 )
        | ((x & (1<<16)) >> 8 )
        | ((x & (1<<18)) >> 9 )
        | ((x & (1<<20)) >> 10 )
        | ((x & (1<<22)) >> 11 )
        | ((x & (1<<24)) >> 12 );
}
int zorder_b(int x)
{
    return zorder_a(x>>1);
}


__kernel 
#if INTEL_PLATFORM == 1
__attribute__((intel_reqd_sub_group_size(8)))
#endif
#if EXTRA_DIM == 0
__attribute__((reqd_work_group_size(BLOCKS_IN_TILE_M, BLOCKS_IN_TILE_N, 1)))
#else
__attribute__((reqd_work_group_size(1,BLOCKS_IN_TILE_M, BLOCKS_IN_TILE_N)))
#endif
void    sgemm(    
#if BATCH_GEMM == 1
        int batches,
#endif        
        int M,int N,int K,
        __global const float * restrict A,ulong offset_A,
#if BATCH_GEMM == 1
        int batch_stride_a,
#endif                
        int lda,
        __global const float * restrict B,ulong offset_B,
#if BATCH_GEMM == 1
        int batch_stride_b,
#endif                
        int ldb,
        __global float * restrict C,ulong offset_C,
#if BATCH_GEMM == 1
        int batch_stride_c,
#endif                
        int ldc,
        float beta_factor
#if BIAS != 0
        , __global const float * restrict bias,ulong offset_bias
#endif
        )
{
    A += offset_A;
    B += offset_B;
    C += offset_C;
#if BATCH_GEMM == 1 
    int batch_id = get_global_id(DIM_G);
    if(batch_id >= batches)
        return;
    A += batch_stride_a * batch_id;
    B += batch_stride_b * batch_id;
    C += batch_stride_c * batch_id;
#endif    

#if CONVGEMM > 0 && GROUPS > 1
    if(get_global_id(DIM_G) >= REDUCE_K * GROUPS)
        return;
    int group = get_global_id(DIM_G) / REDUCE_K;
    #if CONVGEMM == 1
        A += M*K*group;
        B += SRC_COLS*SRC_ROWS*CHANNELS_IN*group;
        C += (IM2COL_OCHAN) * M *group;
        #if BIAS != 0
        bias += M*group;
        #endif
    #elif CONVGEMM == 2
        // M = channels_out / groups
        int step_g_y = (M*IM2COL_OCHAN);
        int step_g_x = (SRC_COLS*SRC_ROWS) * CHANNELS_IN;
        int step_g_w = M*(CHANNELS_IN*KERN_W*KERN_H);
        
        A += step_g_y * group;
        B += step_g_x * group;
        C += step_g_w * group;
    #elif CONVGEMM == 3
        // K = channels_out / group
        int step_g_y = (K*IM2COL_OCHAN);
        int step_g_x = (SRC_COLS*SRC_ROWS) * CHANNELS_IN;
        int step_g_w = K*(CHANNELS_IN*KERN_W*KERN_H);
        
        A += step_g_y * group;
        B += step_g_w * group;
        C += step_g_x * group;
    #else
    #error "Invalid CONVGEMM Value"
    #endif
#endif   

#if ZORDER == 1
    int gr_m = get_group_id(DIM_M);
    int gr_n = get_group_id(DIM_N);
    int gr_size_m = get_num_groups(DIM_M);
    int gr_size_n = get_num_groups(DIM_N);
    if(gr_size_m == gr_size_n && popcount(gr_size_m) == 1) {
        int grs  = gr_n * gr_size_m + gr_m;
        gr_n = zorder_a(grs);
        gr_m = zorder_b(grs);
    }
#else
    int gr_m = get_group_id(DIM_M);
    int gr_n = get_group_id(DIM_N);
#endif
    int tile_row0 = gr_m*TILE_SIZE_M;
    int tile_col0 = gr_n*TILE_SIZE_N;

#if ZORDER == 1
    if(tile_row0 >= M || tile_col0 >= N)
        return;
#endif        

    int row = tile_row0 + get_local_id(DIM_M) * BLOCK_SIZE_M;
    int col = tile_col0 + get_local_id(DIM_N) * BLOCK_SIZE_N;


    int lid0 = get_local_id(DIM_M);
    int lid1 = get_local_id(DIM_N);
    
    int local_tile_id = lid0 * get_local_size(DIM_N) + lid1;

    #define local_wg_size (BLOCKS_IN_TILE_M * BLOCKS_IN_TILE_N)
    #define load_step (TILE_SIZE_M * TILE_SIZE_K / local_wg_size)

    float c[BLOCK_SIZE_M][BLOCK_SIZE_N] = {{0.0f}};
    
    int K_src = K;

#if INTEL_PLATFORM == 0
    float ap[BLOCK_SIZE_M];
    float bp[BLOCK_SIZE_N];

    ALIGN_FLOAT4 __local float a_tile[TILE_SIZE_K][BLOCKS_IN_TILE_M][BLOCK_SIZE_M+TILE_OFFSET];
    ALIGN_FLOAT4 __local float b_tile[TILE_SIZE_K][BLOCKS_IN_TILE_N][BLOCK_SIZE_N+TILE_OFFSET];
#else
    #if ATRANS == 1
    float a[TILE_SIZE_K][BLOCK_SIZE_M];
    #define pA(ind1,ind2) (a[(ind2)][(ind1)])
    #else
    float a[BLOCK_SIZE_M][TILE_SIZE_K];
    #define pA(ind1,ind2) (a[(ind1)][(ind2)])
    #endif
#endif    

#if REDUCE_K > 1
    int KS = (K + REDUCE_K - 1) / REDUCE_K;
    int sec = get_global_id(DIM_G) % REDUCE_K;
    int k_start=KS * sec;
    K = min(K_src,KS * (sec + 1));
    int k = k_start;
#else
    int k=0;
#endif

#if TILE_SIZE_N == TILE_SIZE_M && TILE_SIZE_K % load_step  == 0 && load_step <= TILE_SIZE_K
#define LOAD_VARIANT 0
#else
#define LOAD_VARIANT 1
#endif

#if INTEL_PLATFORM == 0

    #if LOAD_VARIANT == 1
    int dM[load_step];
    int dN[load_step];
    int dK [load_step];
    __local float *aP[load_step];
    __local float *bP[load_step];

    for(int i=0,read_pos = local_tile_id;i<load_step;i++,read_pos+=WG_SIZE) {
        int tile_kdir = read_pos / TILE_SIZE_M;
        int tile_tdir = read_pos % TILE_SIZE_M;
        dM[i] = tile_tdir + tile_row0;
        dN[i] = tile_tdir + tile_col0;
        dK[i]  = tile_kdir;
        aP[i] = &lA(tile_kdir,tile_tdir);
        bP[i] = &lB(tile_kdir,tile_tdir);
    }
    #endif


 
    for(;k<K;k+=TILE_SIZE_K) {


        #if LOAD_VARIANT == 0
        {
            int tile_kdir0 = local_tile_id / TILE_SIZE_M;
            int tile_tdir  = local_tile_id % TILE_SIZE_M;
            int a_row = tile_tdir + tile_row0;
            int b_col = tile_tdir + tile_col0;

            if(a_row >= M) {
                #pragma unroll
                for(int i=0,tile_kdir=tile_kdir0;i<load_step;i++,tile_kdir+=WG_SIZE / TILE_SIZE_M) {
                    lA(tile_kdir,tile_tdir) = 0.0f;
                }
            }
            else {
                if(tile_kdir0 + k <= K - load_step * (WG_SIZE / TILE_SIZE_M)) {
                    #pragma unroll
                    for(int i=0,tile_kdir=tile_kdir0;i<load_step;i++,tile_kdir+=WG_SIZE / TILE_SIZE_M) {
                        int k_rc  = tile_kdir + k;
                        lA(tile_kdir,tile_tdir) = get_A(a_row,k_rc);
                    }
                }
                else {
                    #pragma unroll
                    for(int i=0,tile_kdir=tile_kdir0;i<load_step;i++,tile_kdir+=WG_SIZE / TILE_SIZE_M) {
                        int k_rc  = tile_kdir + k;
                        lA(tile_kdir,tile_tdir) = k_rc < K ? get_A(a_row,k_rc) : 0.0f;
                    }
                }
            }
            if(b_col >= N) {
                #pragma unroll
                for(int i=0,tile_kdir=tile_kdir0;i<load_step;i++,tile_kdir+=WG_SIZE / TILE_SIZE_M) {
                    lB(tile_kdir,tile_tdir) = 0.0f;
                }
            }
            else {
                if(tile_kdir0 + k <= K - load_step * (WG_SIZE / TILE_SIZE_N)) {
                    #pragma unroll
                    for(int i=0,tile_kdir=tile_kdir0;i<load_step;i++,tile_kdir+=WG_SIZE / TILE_SIZE_N) {
      )kern_src"  R"kern_src(                  int k_rc  = tile_kdir + k;
                        lB(tile_kdir,tile_tdir) = get_B(k_rc,b_col);
                    }
                }
                else {
                    #pragma unroll
                    for(int i=0,tile_kdir=tile_kdir0;i<load_step;i++,tile_kdir+=WG_SIZE / TILE_SIZE_N) {
                        int k_rc  = tile_kdir + k;
                        lB(tile_kdir,tile_tdir) = k_rc < K ? get_B(k_rc,b_col) : 0.0f;
                    }
                }
            }

            barrier(CLK_LOCAL_MEM_FENCE);
        }
        #else
        {
            if(tile_row0 + TILE_SIZE_M <= M && k + TILE_SIZE_K <= K) {
                #pragma unroll
                for(int i=0;i<load_step;i++) {
                    int a_row = dM[i];
                    int k_rc  = dK[i] + k;
                    *aP[i] =  get_A(a_row,k_rc);
                }
            }
            else {
                #pragma unroll
                for(int i=0;i<load_step;i++) {
                    int a_row = dM[i];
                    int k_rc  = dK[i] + k;
                    *aP[i] = (a_row < M && k_rc < K) ?  get_A(a_row,k_rc) : 0.0f;
                }
            }
            if(tile_col0 + TILE_SIZE_N <= N && k + TILE_SIZE_K <= K) {
                #pragma unroll
                for(int i=0;i<load_step;i++) {
                    int k_rc  = dK[i]  + k;
                    int b_col = dN[i];
                    *bP[i] = get_B(k_rc,b_col);
                }
            }
            else {
                #pragma unroll
                for(int i=0;i<load_step;i++) {
                    int k_rc  = dK[i]  + k;
                    int b_col = dN[i];
                    *bP[i] = (b_col < N && k_rc < K) ? get_B(k_rc,b_col) : 0.0f;

                }
            }
            barrier(CLK_LOCAL_MEM_FENCE);
        }
        #endif

        // Mutliplication loop
        #pragma unroll(4)
        for(int dk=0;dk<TILE_SIZE_K;dk++) {
            #pragma unroll
            for(int dr=0;dr<BLOCK_SIZE_M;dr++) {
                ap[dr] = a_tile[dk][lid0][dr];
            }
            #pragma unroll
            for(int dc=0;dc<BLOCK_SIZE_N;dc++) {
                bp[dc] = b_tile[dk][lid1][dc];
            }
            #pragma unroll
            for(int dr=0;dr<BLOCK_SIZE_M;dr++) {
                #pragma unroll
                for(int dc=0;dc<BLOCK_SIZE_N;dc++) {
                    c[dr][dc] = mad(ap[dr],bp[dc],c[dr][dc]);
                }
            }
        }

        barrier(CLK_LOCAL_MEM_FENCE);
    }
#else // INTEL_PLATFORM == 1
    // for intel we don't use local memory
    // we use optimized loads from global memory for A
    // and intel_sub_group_shuffle for optimal loading B
    for(;k<K;k+=TILE_SIZE_K) {
        if(row + BLOCK_SIZE_M - 1 < M && k + TILE_SIZE_K-1 < K) {
            #if CONVGEMM == 2 || CONVGEMM == 3
                #pragma unroll
                for(int dr=0;dr<BLOCK_SIZE_M;dr++){
                    for(int dk=0;dk < TILE_SIZE_K;dk++) {
                        pA(dr,dk)=get_A(row+dr,k+dk);
                    }
                }
            #else
                #if ATRANS == 0
                    #pragma unroll
                    for(int dr=0;dr<BLOCK_SIZE_M;dr++){
                        floatK v=vloadK(0,&get_A(row+dr,k));
                        vstoreK(v,0,a[dr]);
                    }
                #else // ATRANS
                    #pragma unroll
                    for(int dk=0;dk<TILE_SIZE_K;dk++){
                        floatM v=vloadM(0,&get_A(row,k+dk));
                        vstoreM(v,0,a[dk]);
                    }
                #endif
            #endif
        }
        else {
            #pragma unroll
            for(int dr=0;dr<BLOCK_SIZE_M;dr++){
                #pragma unroll
                for(int dk=0;dk < TILE_SIZE_K;dk++) {
                    pA(dr,dk) = (row + dr < M && k+dk < K) ? get_A(row+dr,k+dk): 0;
                }
            }
        }

        #pragma unroll(TILE_SIZE_K)
        for(int dk=0;dk<TILE_SIZE_K;dk++) {
            if(k + dk >= K)
                continue;
            #if BLOCK_SIZE_N == 8
                int mycol = col + get_sub_group_local_id();
                float myv = (mycol < N) ? get_B(k+dk,col + get_sub_group_local_id()) : 0;
                #pragma unroll
                for(int dc=0;dc<BLOCK_SIZE_N;dc++){
                    float b_dc = intel_sub_group_shuffle(myv,dc);
                    for(int dr=0;dr<BLOCK_SIZE_M;dr++) {
                        c[dr][dc] = mad(pA(dr,dk),b_dc,c[dr][dc]);
                    }
                }
            #else
                #pragma unroll
                for(int dc=0;dc<BLOCK_SIZE_N;dc++){
                    float b_dc = (col + dc < N) ? get_B(k+dk,col+dc) : 0;
                    for(int dr=0;dr<BLOCK_SIZE_M;dr++) {
                        c[dr][dc] = mad(pA(dr,dk),b_dc,c[dr][dc]);
                    }
                }
            #endif
        }
    }
#endif // INTEL_PLATFORM = 1


#if BIAS != 0
    bias += offset_bias;
#endif

#if BIAS == 1
    #if REDUCE_K > 1
    if(k_start == 0)
    #endif
    {
        float offset;
        #pragma unroll
        for(int dr=0;dr<BLOCK_SIZE_M;dr++) {
            offset = row + dr < M ? bias[(row+dr)] : 0.0f;
            #pragma unroll
            for(int dc=0;dc<BLOCK_SIZE_N;dc++) {
                c[dr][dc] += offset;
            }
        }
    }
#elif BIAS == 2
    #if REDUCE_K > 1
    if(k_start == 0)
    #endif
    {
        float offset;
        #pragma unroll
        for(int dc=0;dc<BLOCK_SIZE_N;dc++) {
            offset = (col + dc) < N ? bias[(col+dc)] : 0.0f;
            #pragma unroll
            for(int dr=0;dr<BLOCK_SIZE_M;dr++) {
                c[dr][dc] += offset;
            }
        }
    }
#endif    

#if CONVGEMM == 1
    {
        #pragma unroll
        for(int dc=0;dc<BLOCK_SIZE_N;dc++) {
            if(col + dc >= N)
                continue;
            int matrix_col = col + dc;
            int batch = matrix_col / IM2COL_OCHAN;
            int incol = matrix_col % IM2COL_OCHAN;
            int offset = batch * (IM2COL_OCHAN * GROUPS) * M + incol;
            #pragma unroll
            for(int dr=0;dr<BLOCK_SIZE_M;dr++) {
                if(row+dr < M) {
                    int index =(row + dr)*ldc + offset;
                    #if REDUCE_K > 1
                    atomic_addf(C+index,c[dr][dc]);
                    #else
                    if(beta_factor != 0)
                        C[index] = mad(C[index], beta_factor,ACTIVATION_F(c[dr][dc]));
                    else
                        C[index] = ACTIVATION_F(c[dr][dc]);
                    #endif
                }
            }
        }
    }
#else
    {
        #pragma unroll
        for(int dr=0;dr<BLOCK_SIZE_M;dr++) {
            #pragma unroll
            for(int dc=0;dc<BLOCK_SIZE_N;dc++) {
                if(row + dr < M && col+dc < N) {
                    #if CONVGEMM == 3
                        add_img_value(C,row+dr,col+dc,c[dr][dc]);
                    #else
                        int index = (row+dr)*ldc+col+dc;
                        #if REDUCE_K > 1
                        atomic_addf(C+index,ACTIVATION_F(c[dr][dc]));
                        #else
                        if(beta_factor != 0)
                            C[index] = mad(C[index], beta_factor,ACTIVATION_F(c[dr][dc]));
                        else
                            C[index] = ACTIVATION_F(c[dr][dc]);
                        #endif
                    #endif
                }
            }
        }
    }
#endif
}


)kern_src" },{"softmax", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#ifndef CUSTOM_REDUCE
#define CUSTOM_REDUCE 0
#endif

#define my_get_local_wg_id() ((get_local_id(2) * get_local_size(1) * get_local_size(0)) + (get_local_id(1) * get_local_size(0)) + get_local_id(0))
#if __OPENCL_VERSION__ >= 200 && !CUSTOM_REDUCE
#define REDUCE_PREPARE(WG_SIZE,dtype) do {} while(0)
#define my_work_group_reduce_add(val) do { val = work_group_reduce_add(val); } while(0)
#define my_work_group_reduce_max(val) do { val = work_group_reduce_max(val); } while(0)
#else

#define REDUCE_PREPARE(WG_SIZE,dtype) __local dtype my_reduce[WG_SIZE];
#define REDUCE_USING_OP(myval,reduce_op) \
    do { \
        int lid = my_get_local_wg_id(); \
        my_reduce[lid] = myval; \
        barrier(CLK_LOCAL_MEM_FENCE); \
        const int WGS = sizeof(my_reduce)/sizeof(my_reduce[0]); \
        for(int i=WGS / 2;i>0; i>>= 1) { \
            if(lid < i) { \
                my_reduce[lid] = reduce_op(my_reduce[lid],my_reduce[lid+i]); \
            } \
            barrier(CLK_LOCAL_MEM_FENCE); \
        } \
        myval = my_reduce[0]; \
    } while(0)

#define REDUCE_OP_ADD(x,y) ((x) + (y))
#define REDUCE_OP_MAX(x,y) max((x),(y))

#define my_work_group_reduce_add(val) REDUCE_USING_OP(val,REDUCE_OP_ADD)
#define my_work_group_reduce_max(val) REDUCE_USING_OP(val,REDUCE_OP_MAX)

#endif

#ifndef WG_SIZE
#define WG_SIZE 256
#endif

#ifndef ITEMS_PER_WI
#define ITEMS_PER_WI 1
#endif

#ifndef LOG_SM
#define LOG_SM 0
#endif

#ifndef CALC_LOSS
#define CALC_LOSS 0
#endif

#if CALC_LOSS==1
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
void atomic_addf(__global volatile float *ptr,float v)
{
#if defined(__opencl_c_ext_fp32_global_atomic_add)
    atomic_fetch_add((__global volatile atomic_float *)ptr,v);
#elif defined(cl_intel_subgroups)
    __global atomic_int *p = (__global atomic_int *)(ptr);
    int prev,newv;
    do {
        prev = atomic_load(p);
        newv = as_int(as_float(prev) + v);
    } while(! atomic_compare_exchange_weak(p,&prev,newv));
#elif defined(__NV_CL_C_VERSION)
    float prev;
    asm volatile(
        "atom.global.add.f32 %0, [%1], %2;"
        : "=f"(prev)
        : "l"(ptr) , "f"(v)
        : "memory"
    );
#else
    float oldv = *ptr;
    for(;;) {
        float newv = oldv + v;
        int prev = atomic_cmpxchg((__global volatile int *)(ptr),as_int(oldv),as_int(newv));
        if(prev == as_int(oldv))
            return;
        oldv = as_float(prev);
    }
#endif    
}

#endif



#define LOCAL_ITEMS_LIMIT 32
__kernel 
__attribute__((reqd_work_group_size(1,WG_SIZE,1)))
void softmax(int batch,int channels,int extra_batch,
             __global dtype const *in,ulong  data_offset,
             __global dtype *out,ulong  out_offset)
{
    in += data_offset;
    out += out_offset;
    
    long b = get_global_id(0);
    long eb = get_global_id(2);
    long step = extra_batch;

    if(b >= batch)
        return;
    if(eb >= extra_batch)
        return;

    int c = get_global_id(1) * ITEMS_PER_WI;

    in += b * channels * extra_batch + eb;
    out += b * channels * extra_batch + eb;
    
    REDUCE_PREPARE(WG_SIZE,dtype);

    dtype val = -DTYPE_MAX;

    #if ITEMS_PER_WI <= LOCAL_ITEMS_LIMIT
        dtype values[ITEMS_PER_WI];
        #pragma unroll
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c+i < channels) {
                values[i] = in[(c+i)*step];
                val = max(val,values[i]);
            }
        }
    #else
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c+i < channels) {
                val = max(val,in[(c+i)*step]);
            }
        }
    #endif


    my_work_group_reduce_max(val);
    dtype maxv = val;

    dtype sum = 0;

    #if ITEMS_PER_WI <= LOCAL_ITEMS_LIMIT
        #pragma unroll
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c+i < channels) {
                #if LOG_SM == 1
                sum += exp(values[i] - maxv);
                #else
                sum += values[i] = exp(values[i] - maxv);
                #endif
            }
        }
    #else
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c+i < channels) {
                dtype tmp = exp(in[(c+i)*step] - maxv);
                #if LOG_SM == 0
                out[(c+i)*step] = tmp;
                #endif
                sum += tmp;
            }
        }
    #endif
    my_work_group_reduce_add(sum);

    #if LOG_SM == 0
    val = (dtype)1 / sum;
    #else
    val = -log(sum);
    #endif

    #if ITEMS_PER_WI <= LOCAL_ITEMS_LIMIT
        #pragma unroll
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c + i < channels) {
                #if LOG_SM == 1
                out[(c+i)*step] = values[i] - maxv + val;
                #else
                out[(c+i)*step] = values[i] * val;
                #endif
            }
        }
    #else
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c + i < channels) {
                #if LOG_SM == 1
                out[(c+i)*step] = in[(c+i)*step] - maxv + val;
                #else
                out[(c+i)*step] *= val;
                #endif
            }
        }
    #endif
}


__kernel 
__attribute__((reqd_work_group_size(1,WG_SIZE,1)))
void softmax_backward(int batch,int channels,int extra_batch,
             __global dtype *in,ulong  data_offset,
             __global dtype const *out,ulong  out_offset,
             __global dtype const *out_diff,ulong  out_diff_offset,
             dtype factor
             )
{
    in += data_offset;
    out += out_offset;
    out_diff += out_diff_offset;
    
    long b = get_global_id(0);
    long eb = get_global_id(2);

    if(b >= batch)
        return;
    if(eb >= extra_batch)
        return;
    long step = extra_batch;

    int c = get_global_id(1) * ITEMS_PER_WI;

    in += b * channels * extra_batch + eb;
    out += b * channels * extra_batch + eb;
    out_diff += b * channels * extra_batch + eb;
    
    dtype sum = 0;
    REDUCE_PREPARE(WG_SIZE,dtype);

    #if ITEMS_PER_WI <= LOCAL_ITEMS_LIMIT
    #pragma unroll(ITEMS_PER_WI)
    #else
    #pragma unroll(LOCAL_ITEMS_LIMIT)
    #endif
    for(int i=0;i<ITEMS_PER_WI;i++) {
        if(c+i < channels) {
            #if LOG_SM == 1
            sum += out_diff[(c+i) * step];
            #else
            sum += out_diff[(c+i)*step] * out[(c+i)*step];
            #endif
        }
    }

    my_work_group_reduce_add(sum);

    #if ITEMS_PER_WI <= LOCAL_ITEMS_LIMIT
    #pragma unroll(ITEMS_PER_WI)
    #else
    #pragma unroll(LOCAL_ITEMS_LIMIT)
    #endif
    for(int i=0;i<ITEMS_PER_WI;i++) {
        if(c + i < channels) {
            #if LOG_SM == 1
            float dxval = out_diff[(c+i)*step] - exp(out[(c+i)*step]) * sum;
            #else
            float dxval = (out_diff[(c+i)*step] - sum) * out[(c+i)*step]; 
            #endif
            if(factor == 0)
                in[(c+i)*step] = dxval;
            else
                in[(c+i)*step] = factor * in[(c+i)*step] + dxval;
        }
    }
}

)kern_src" },{"softmax_with_loss", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#ifndef CUSTOM_REDUCE
#define CUSTOM_REDUCE 0
#endif

#define my_get_local_wg_id() ((get_local_id(2) * get_local_size(1) * get_local_size(0)) + (get_local_id(1) * get_local_size(0)) + get_local_id(0))
#if __OPENCL_VERSION__ >= 200 && !CUSTOM_REDUCE
#define REDUCE_PREPARE(WG_SIZE,dtype) do {} while(0)
#define my_work_group_reduce_add(val) do { val = work_group_reduce_add(val); } while(0)
#define my_work_group_reduce_max(val) do { val = work_group_reduce_max(val); } while(0)
#else

#define REDUCE_PREPARE(WG_SIZE,dtype) __local dtype my_reduce[WG_SIZE];
#define REDUCE_USING_OP(myval,reduce_op) \
    do { \
        int lid = my_get_local_wg_id(); \
        my_reduce[lid] = myval; \
        barrier(CLK_LOCAL_MEM_FENCE); \
        const int WGS = sizeof(my_reduce)/sizeof(my_reduce[0]); \
        for(int i=WGS / 2;i>0; i>>= 1) { \
            if(lid < i) { \
                my_reduce[lid] = reduce_op(my_reduce[lid],my_reduce[lid+i]); \
            } \
            barrier(CLK_LOCAL_MEM_FENCE); \
        } \
        myval = my_reduce[0]; \
    } while(0)

#define REDUCE_OP_ADD(x,y) ((x) + (y))
#define REDUCE_OP_MAX(x,y) max((x),(y))

#define my_work_group_reduce_add(val) REDUCE_USING_OP(val,REDUCE_OP_ADD)
#define my_work_group_reduce_max(val) REDUCE_USING_OP(val,REDUCE_OP_MAX)

#endif

#ifndef WG_SIZE
#define WG_SIZE 256
#endif

#ifndef ITEMS_PER_WI
#define ITEMS_PER_WI 1
#endif

#ifndef LOG_SM
#define LOG_SM 0
#endif

#ifndef CALC_LOSS
#define CALC_LOSS 0
#endif

#if CALC_LOSS==1
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
void atomic_addf(__global volatile float *ptr,float v)
{
#if defined(__opencl_c_ext_fp32_global_atomic_add)
    atomic_fetch_add((__global volatile atomic_float *)ptr,v);
#elif defined(cl_intel_subgroups)
    __global atomic_int *p = (__global atomic_int *)(ptr);
    int prev,newv;
    do {
        prev = atomic_load(p);
        newv = as_int(as_float(prev) + v);
    } while(! atomic_compare_exchange_weak(p,&prev,newv));
#elif defined(__NV_CL_C_VERSION)
    float prev;
    asm volatile(
        "atom.global.add.f32 %0, [%1], %2;"
        : "=f"(prev)
        : "l"(ptr) , "f"(v)
        : "memory"
    );
#else
    float oldv = *ptr;
    for(;;) {
        float newv = oldv + v;
        int prev = atomic_cmpxchg((__global volatile int *)(ptr),as_int(oldv),as_int(newv));
        if(prev == as_int(oldv))
            return;
        oldv = as_float(prev);
    }
#endif    
}

#endif



#define LOCAL_ITEMS_LIMIT 32
__kernel 
__attribute__((reqd_work_group_size(1,WG_SIZE,1)))
void softmax(int batch,int channels,
             __global dtype *in,ulong  data_offset,
#if CALC_LOSS==2
             __global dtype *in_diff,ulong  in_diff_offset,
#endif
             __global itype *label,ulong  label_offset,             
             __global dtype *out,ulong  out_offset
#if CALC_LOSS==2
            ,dtype factor
#endif            
             )
{
    in += data_offset;
    out += out_offset;
    
    int b = get_global_id(0);

    if(b >= batch)
        return;

    int c = get_global_id(1) * ITEMS_PER_WI;

    in += b * channels;
    label += label_offset + b;
    #if CALC_LOSS == 2
    in_diff += b * channels + in_diff_offset;
    #endif
    
    REDUCE_PREPARE(WG_SIZE,dtype);

    dtype val = -DTYPE_MAX;
    #if ITEMS_PER_WI <= LOCAL_ITEMS_LIMIT
        dtype values[ITEMS_PER_WI];
        #pragma unroll
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c+i < channels) {
                values[i] = in[c+i];
                val = max(val,values[i]);
            }
        }
    #else
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c+i < channels) {
                val = max(val,in[c+i]);
            }
        }
    #endif


    my_work_group_reduce_max(val);
    dtype maxv = val;

    dtype sum = 0;

    #if ITEMS_PER_WI <= LOCAL_ITEMS_LIMIT
        #pragma unroll
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c+i < channels) {
                sum += values[i] = exp(values[i] - maxv);
            }
        }
    #else
        for(int i=0;i<ITEMS_PER_WI;i++) {
            if(c+i < channels) {
                dtype tmp = exp(in[c+i] - maxv);
                sum += tmp;
            }
        }
    #endif
    my_work_group_reduce_add(sum);

#if CALC_LOSS == 1
    if(get_local_id(1) == 0) {
        int index = *label;
        dtype loss = 0;
        if(0 <= index && index < channels)
            loss = -(in[index] - maxv - log(sum));
        atomic_addf(out,loss / batch);
    }
#elif CALC_LOSS == 2
    val = (dtype)1 / sum;
    int index = *label;
    dtype gr;
    dtype loss = *out / batch;
    
    #pragma unroll(8)
    for(int i=0;i<ITEMS_PER_WI;i++) {
        if(c + i < channels) {
            #if ITEMS_PER_WI <= LOCAL_ITEMS_LIMIT
            dtype sm_val = values[i];
            #else
            dtype sm_val = exp(in[c+i]-maxv);
            #endif
            gr = loss *( sm_val * val - (int)(index ==  c + i));
            if(factor == 0)
                in_diff[c+i] = gr;
            else
                in_diff[c+i] = in_diff[c+i] * factor + gr;
        }
    }
#endif
}

)kern_src" },{"winograd_bwd_data", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
void atomic_addf(__global volatile float *ptr,float v)
{
#if defined(__opencl_c_ext_fp32_global_atomic_add)
    atomic_fetch_add((__global volatile atomic_float *)ptr,v);
#elif defined(cl_intel_subgroups)
    __global atomic_int *p = (__global atomic_int *)(ptr);
    int prev,newv;
    do {
        prev = atomic_load(p);
        newv = as_int(as_float(prev) + v);
    } while(! atomic_compare_exchange_weak(p,&prev,newv));
#elif defined(__NV_CL_C_VERSION)
    float prev;
    asm volatile(
        "atom.global.add.f32 %0, [%1], %2;"
        : "=f"(prev)
        : "l"(ptr) , "f"(v)
        : "memory"
    );
#else
    float oldv = *ptr;
    for(;;) {
        float newv = oldv + v;
        int prev = atomic_cmpxchg((__global volatile int *)(ptr),as_int(oldv),as_int(newv));
        if(prev == as_int(oldv))
            return;
        oldv = as_float(prev);
    }
#endif    
}


/***
 * Implementation concept is based on the
 *
 * Yan, Da, Wei Wang, and Xiaowen Chu.
 * "Optimizing batched winograd convolution on GPUs."
 * Proceedings of the 25th ACM SIGPLAN symposium on
 * principles and practice of parallel programming. 2020.
 *
 * @inproceedings{yan2020optimizing,
 *      title={Optimizing batched winograd convolution on GPUs},
 *      author={Yan, Da and Wang, Wei and Chu, Xiaowen},
 *      booktitle={Proceedings of the 25th ACM SIGPLAN symposium on principles and practice of parallel programming},
 *      pages={32--44},
 *      year={2020}
 * }
 *
 *
 * https://www.cse.ust.hk/~weiwa/papers/yan-ppopp20.pdf
 *
 * In comparison to the paper: using 32x8 tiles block instead of 64x8
 * since it generic OpenCL implementation for different GPUs
 * that can't optimize registers as efficienlty as manually written
 * assembly
 */

void transform_tile_bwd(float16 inp,float babt[4][4])
{
    // B = 
    // [[ 1.  0.  0.  0.]
    //  [ 0.  1. -1.  1.]
    //  [-1.  1.  1.  0.]
    //  [ 0.  0.  0. -1.]]
    //
    // compute B*a*B'

    float4 a[4] = { inp.lo.lo, inp.lo.hi, inp.hi.lo, inp.hi.hi };

    //
    float4 ba[4];

    ba[0] =  a[0];
    ba[1] =  a[1] - a[2] + a[3];
    ba[2] =  a[1] + a[2] - a[0];
    ba[3] = -a[3];

    #pragma unroll
    for(int i=0;i<4;i++) {
        babt[i][0] =  ba[i].s0;
        babt[i][1] =  ba[i].s1 - ba[i].s2 + ba[i].s3;
        babt[i][2] =  ba[i].s1 + ba[i].s2 - ba[i].s0;
        babt[i][3] = -ba[i].s3;
    }
}


float16 load_3x3_kernel_and_transform(__global const float *kern_ptr)
{
    float4 gk[3];
    float k[9];
    
    #pragma unroll
    for(int i=0;i<9;i++)
        k[i]=kern_ptr[i];

    gk[0].s0 = k[0];
    gk[1].s0 = k[1];
    gk[2].s0 = k[2];

    gk[0].s1 = 0.5f * (k[0] + k[3] + k[6]);
    gk[1].s1 = 0.5f * (k[1] + k[4] + k[7]);
    gk[2].s1 = 0.5f * (k[2] + k[5] + k[8]);

    gk[0].s2 = 0.5f * (k[0] - k[3] + k[6]);
    gk[1].s2 = 0.5f * (k[1] - k[4] + k[7]);
    gk[2].s2 = 0.5f * (k[2] - k[5] + k[8]);

    gk[0].s3 = k[6];
    gk[1].s3 = k[7];
    gk[2].s3 = k[8];

    float16 k4;

    k4.s048c = gk[0];
    k4.s159d = 0.5f * (gk[0] + gk[1] + gk[2]);
    k4.s26ae = 0.5f * (gk[0] - gk[1] + gk[2]);
    k4.s37bf = gk[2];
    return k4;
}

float16 tile2x2_to_4x4(float4 v)
{
    //[ 1.  0.]
    //[ 1.  1.]
    //[ 1. -1.]
    //[ 0. -1.]

    float y[2][2] = { { v.s0, v.s1 }, {v.s2, v.s3} };
    float Av[4][2];
    #pragma unroll
    for(int dc=0;dc<2;dc++) {
        Av[0][dc] = y[0][dc];
        Av[1][dc] = y[0][dc] + y[1][dc];
        Av[2][dc] = y[0][dc] - y[1][dc];
        Av[3][dc] =          - y[1][dc];
    }
    // A'
    // 1  1  1  0 
    // 0  1 -1 -1 
    float4 AvAT[4];
    #pragma unroll
    for(int dr=0;dr<4;dr++) {
        AvAT[dr].s0 = Av[dr][0];
        AvAT[dr].s1 = Av[dr][0] + Av[dr][1];
        AvAT[dr].s2 = Av[dr][0] - Av[dr][1];
        AvAT[dr].s3 =           - Av[dr][1];
    }
    return (float16)(AvAT[0],AvAT[1],AvAT[2],AvAT[3]);
}


float16 load_2x2_tile_and_transform(__global const float * restrict frame,int pos,int limit,int end,int stride,int2 mask[2])
{
    float2 a[2];
    frame += pos;
    
    if(pos < limit) {
        #pragma unroll
        for(int i=0;i<2;i++,frame+=stride) {
            a[i] = as_float2(as_int2(vload2(0,frame)) & mask[i]);
        }
    }
    else {
        #pragma unroll
        for(int i=0;i<2;i++,frame+=stride,pos+=stride) {
            if(pos + 2 <= end)
               a[i] = as_float2(as_int2(vload2(0,frame)) & mask[i]);
            else {
                float2 tmp;
                tmp.s0 = pos+0 < end ? frame[0] : 0.0;
                tmp.s1 = pos+1 < end ? frame[1] : 0.0;
                a[i] = as_float2(as_int2(tmp) & mask[i]);
            }
        }
    }
    return tile2x2_to_4x4((float4)(a[0],a[1]));
}


__kernel void winconv_calc_gkgt_3x3(int N,int C,
                                    __global const float * restrict gk3,
                                    ulong gk3_offset,
                                    __global float16 *k4,
                                    ulong k4_offset)
{
    gk3 += gk3_offset;
    k4 += k4_offset / 16;
    int n = get_global_id(0);
    int c = get_global_id(1);
    if(n >= N || c>= C)
        return;
    float16 kern = load_3x3_kernel_and_transform(gk3 + (C * n + c) * 9);
    k4[C * n + c] = kern;
}



inline void store_local(__local float *l_val,int strd,float16 v)
{
        l_val[ 0*strd] = v.s0;
        l_val[ 1*strd] = v.s1;
        l_val[ 2*strd] = v.s2;
        l_val[ 3*strd] = v.s3;
        l_val[ 4*strd] = v.s4;
        l_val[ 5*strd] = v.s5;
        l_val[ 6*strd] = v.s6;
        l_val[ 7*strd] = v.s7;
        l_val[ 8*strd] = v.s8;
        l_val[ 9*strd] = v.s9;
        l_val[10*strd] = v.sa;
        l_val[11*strd] = v.sb;
        l_val[12*strd] = v.sc;
        l_val[13*strd] = v.sd;
        l_val[14*strd] = v.se;
        l_val[15*strd] = v.sf;
}

#define WG_SIZE 256
#define TILES_IN_WG 32
#define KERNELS_IN_WG 32
#define WG_K 8

#if  WG_K * TILES_IN_WG  != WG_SIZE || KERNELS_IN_WG * WG_K != WG_SIZE
#error "Parameters do not match"
#endif

#define WG_DIM 0

#define PAD_H 1
#define PAD_W 1

#define PATCH_K 8
#define PATCH_T 8

#if TILES_IN_WG * KERNELS_IN_WG * 16 != PATCH_K * PATCH_T * WG_SIZE
#error
#endif

#ifndef TR_STRIDE_OFFSET
#define TR_STRIDE_OFFSET 1
#endif

#if STRIDE_OFFSET > 0 || TR_STRIDE_OFFSET > 0
#define PADDING_FACTOR 1
#else
#define PADDING_FACTOR 0
#endif


__kernel 
__attribute__((reqd_work_group_size(WG_SIZE,1,1)))
void winconv_3x3_bwd_data(int B, int N,int C,int H,int W,
                          __global float const * restrict image,ulong image_offset,
                          __global float16 const * restrict kernels,ulong kernels_offset,
                          __global float *restrict result,ulong result_offset
                          )
{
    image += image_offset;
    kernels += kernels_offset / 16;
    result += result_offset;

    int half_W = (W+1)/2;
    int half_H = (H+1)/2;
    int half_WG = half_W * half_H;

    __local float wg_local_memory[(TILES_IN_WG + KERNELS_IN_WG + 16 * PADDING_FACTOR) * WG_K * 16];

#define l_tile_stride (TILES_IN_WG * WG_K + STRIDE_OFFSET)
#define l_kern_stride (KERNELS_IN_WG * WG_K + STRIDE_OFFSET)

#define l_tiles(a,b,c) wg_local_memory[(a)*l_tile_stride + (b)*TILES_IN_WG + (c)]
#define l_kernels(a,b,c) wg_local_memory[(a)*l_kern_stride + (b)*KERNELS_IN_WG + (c) + (TILES_IN_WG*WG_K*16 + 32 * STRIDE_OFFSET )]


    // Loading data
    int l_tile_rc = get_local_id(WG_DIM) % TILES_IN_WG;
    int l_tile_k  = get_local_id(WG_DIM) / TILES_IN_WG;

    int l_kern_n  = get_local_id(WG_DIM) % KERNELS_IN_WG;
    int l_kern_k  = get_local_id(WG_DIM) / KERNELS_IN_WG;
   
    int wg_brc    = get_group_id(0) * TILES_IN_WG;
    int wg_channel= get_global_id(1) * KERNELS_IN_WG; 

    int l_brc     = wg_brc + l_tile_rc;
    int l_channel = wg_channel + l_kern_n;

    int l_b   = l_brc / half_WG;
    int l_rc  = l_brc % half_WG;
    int l_row = l_rc / half_W * 2;
    int l_col = l_rc % half_W * 2;

    int result_pos = l_b * H * W * N + l_row * W + l_col + (l_tile_k) * (H * W);
    int result_end   = B * N * W * H;
    int result_step  = H*W*WG_K;
    int result_limit = result_end - W*2;
    int2 load_mask[2];
    #pragma unroll
    for(int dr=0;dr<2;dr++) {
        if(l_row + dr >= H) {
            load_mask[dr] = 0;
        }
        else {
            int c=l_col;
            load_mask[dr].s0 = c < W ? -1 : 0;
            c++;
            load_mask[dr].s1 = c < W ? -1 : 0;
            c++;
        }
    }

    #define s_img_tile(k,t,indx) wg_local_memory[(k)*(TILES_IN_WG/2*16 + TR_STRIDE_OFFSET) + (t/2) * 16 + (indx)]
    
    __local float *l_tile_ptr = &l_tiles(0,l_tile_k,l_tile_rc);
    __local float *l_kern_ptr = &l_kernels(0,l_kern_k,l_kern_n);

    // GEMM

    int my_gemm_tile_b  = get_local_id(WG_DIM) / 16;
    int my_gemm_tile_tk = get_local_id(WG_DIM) % 16;
    int my_gemm_tile_kr = my_gemm_tile_tk / (TILES_IN_WG / PATCH_T) * PATCH_K;
    int my_gemm_tile_tl = my_gemm_tile_tk % (TILES_IN_WG / PATCH_T) * PATCH_T;

    float p_C[PATCH_K][PATCH_T]={{0}};

    // store

    for(int k=0;k < N;k+=WG_K,result_pos += WG_K*W*H)
    {

        // load relevant tile       
        float16 my_tile = (l_b < B && k + l_tile_k < N) ? load_2x2_tile_and_transform(result,result_pos,result_limit,result_end,W,load_mask) : 0; 
        store_local(l_tile_ptr,l_tile_stride,my_tile);
        
        // load relevant kernel
        float16 my_kern = (l_channel < C && k+l_kern_k < N) ? kernels[(l_kern_k + k) * C + l_channel] : 0;
        store_local(l_kern_ptr,l_kern_stride,my_kern);

        barrier(CLK_LOCAL_MEM_FENCE);

        // GEMM
        #pragma unroll
        for(int dk=0;dk<WG_K;dk++) {
            float p_kern[PATCH_K];

            #pragma unroll
            for(int dr=0;dr<PATCH_K;dr++) {
                p_kern[dr] = l_kernels(my_gemm_tile_b,dk,dr + my_gemm_tile_kr);
            }
            float p_tile[PATCH_T];
            #pragma unroll
            for(int dc=0;dc<PATCH_T;dc++) {
                p_tile[dc] = l_tiles(my_gemm_tile_b,dk,dc + my_gemm_tile_tl);
            }
            #pragma unroll
            for(int dr=0;dr<PATCH_K;dr++) {
                #pragma unroll
                for(int dc=0;dc<PATCH_T;dc++) {
                    p_C[dr][dc] += p_kern[dr]*p_tile[dc];
                }
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    int tile0 = get_local_id(WG_DIM) * 4;
    int s_col_t0 = tile0 % TILES_IN_WG;
    int s_row_t = tile0 / TILES_IN_WG;

    #pragma unroll
    for(int dc_split = 0; dc_split < 2;dc_split++) {
        // transpose part A

        #pragma unroll
        for(int dr=0;dr<PATCH_K;dr++) {
            int tile_r = dr + my_gemm_tile_kr;
            #pragma unroll
            for(int dc=0;dc<PATCH_T;dc+=2) {
                int tile_c = dc + dc_split + my_gemm_tile_tl;
                s_img_tile(tile_r,tile_c,my_gemm_tile_b) = p_C[dr][dc + dc_split];
             }
        }
        
        barrier(CLK_LOCAL_MEM_FENCE);

        #pragma unroll
        for(int dc = 0;dc < 4;dc+=2) {
            
            int s_col_t = s_col_t0 + dc + dc_split;

            float16 s_tile = vload16(0,&s_img_tile(s_row_t,s_col_t,0));
            float s_img_tile[4][4];
            transform_tile_bwd(s_tile,s_img_tile);

            int s_brc = wg_brc + s_col_t;

            int s_b   = s_brc / half_WG;
            int s_rc  = s_brc % half_WG;
            int s_row = s_rc / half_W * 2 - 1;
            int s_col = s_rc % half_W * 2 - 1;

            int s_channel = wg_channel + s_row_t;

            if(s_b < B && s_channel < C ) {
                __global float *ptr = image + ((s_b * C + s_channel) * H + s_row) * W + s_col;
                #pragma unroll
                for(int dr=0;dr<4;dr++,ptr+=W) {
                    int r = s_row + dr;
                    if(r >= 0 && r<H) {
                        #pragma unroll
                        for(int dc=0;dc<4;dc++) {
                            int c = s_col + dc;
                            if(c >= 0 && c < W) {
                                atomic_addf(ptr + dc,s_img_tile[dr][dc]);
                            }
                        }
                    }
                }
            }
        }
        
        barrier(CLK_LOCAL_MEM_FENCE);
    }

}


)kern_src" },{"winograd_bwd_filter", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 


///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
void atomic_addf(__global volatile float *ptr,float v)
{
#if defined(__opencl_c_ext_fp32_global_atomic_add)
    atomic_fetch_add((__global volatile atomic_float *)ptr,v);
#elif defined(cl_intel_subgroups)
    __global atomic_int *p = (__global atomic_int *)(ptr);
    int prev,newv;
    do {
        prev = atomic_load(p);
        newv = as_int(as_float(prev) + v);
    } while(! atomic_compare_exchange_weak(p,&prev,newv));
#elif defined(__NV_CL_C_VERSION)
    float prev;
    asm volatile(
        "atom.global.add.f32 %0, [%1], %2;"
        : "=f"(prev)
        : "l"(ptr) , "f"(v)
        : "memory"
    );
#else
    float oldv = *ptr;
    for(;;) {
        float newv = oldv + v;
        int prev = atomic_cmpxchg((__global volatile int *)(ptr),as_int(oldv),as_int(newv));
        if(prev == as_int(oldv))
            return;
        oldv = as_float(prev);
    }
#endif    
}


/***
 * Implementation concept is based on the
 *
 * Yan, Da, Wei Wang, and Xiaowen Chu.
 * "Optimizing batched winograd convolution on GPUs."
 * Proceedings of the 25th ACM SIGPLAN symposium on
 * principles and practice of parallel programming. 2020.
 *
 * @inproceedings{yan2020optimizing,
 *      title={Optimizing batched winograd convolution on GPUs},
 *      author={Yan, Da and Wang, Wei and Chu, Xiaowen},
 *      booktitle={Proceedings of the 25th ACM SIGPLAN symposium on principles and practice of parallel programming},
 *      pages={32--44},
 *      year={2020}
 * }
 *
 *
 * https://www.cse.ust.hk/~weiwa/papers/yan-ppopp20.pdf
 *
 * In comparison to the paper: using 32x8 tiles block instead of 64x8
 * since it generic OpenCL implementation for different GPUs
 * that can't optimize registers as efficienlty as manually written
 * assembly
 */

float16 tile2x2_to_4x4(float4 v)
{
    //[ 1.  0.]
    //[ 1.  1.]
    //[ 1. -1.]
    //[ 0. -1.]

    float y[2][2] = { { v.s0, v.s1 }, {v.s2, v.s3} };
    float Av[4][2];
    #pragma unroll
    for(int dc=0;dc<2;dc++) {
        Av[0][dc] = y[0][dc];
        Av[1][dc] = y[0][dc] + y[1][dc];
        Av[2][dc] = y[0][dc] - y[1][dc];
        Av[3][dc] =          - y[1][dc];
    }
    // A'
    // 1  1  1  0 
    // 0  1 -1 -1 
    float4 AvAT[4];
    #pragma unroll
    for(int dr=0;dr<4;dr++) {
        AvAT[dr].s0 = Av[dr][0];
        AvAT[dr].s1 = Av[dr][0] + Av[dr][1];
        AvAT[dr].s2 = Av[dr][0] - Av[dr][1];
        AvAT[dr].s3 =           - Av[dr][1];
    }
    return (float16)(AvAT[0],AvAT[1],AvAT[2],AvAT[3]);
}

float16 transform_tile(float4 a[4])
{
    float bta[4][4];

    bta[0][0] = a[0].s0 - a[2].s0;
    bta[0][1] = a[0].s1 - a[2].s1;
    bta[0][2] = a[0].s2 - a[2].s2;
    bta[0][3] = a[0].s3 - a[2].s3;

    bta[1][0] = a[1].s0 + a[2].s0;
    bta[1][1] = a[1].s1 + a[2].s1;
    bta[1][2] = a[1].s2 + a[2].s2;
    bta[1][3] = a[1].s3 + a[2].s3;

    bta[2][0] = a[2].s0 - a[1].s0;
    bta[2][1] = a[2].s1 - a[1].s1;
    bta[2][2] = a[2].s2 - a[1].s2;
    bta[2][3] = a[2].s3 - a[1].s3;

    bta[3][0] = a[1].s0 - a[3].s0;
    bta[3][1] = a[1].s1 - a[3].s1;
    bta[3][2] = a[1].s2 - a[3].s2;
    bta[3][3] = a[1].s3 - a[3].s3;

    float4 btab[4];
    #pragma unroll
    for(int i=0;i<4;i++) {
        btab[i].s0 = bta[i][0] - bta[i][2];
        btab[i].s1 = bta[i][1] + bta[i][2];
        btab[i].s2 = bta[i][2] - bta[i][1];
        btab[i].s3 = bta[i][1] - bta[i][3];
    }
    
    return (float16)(btab[0],btab[1],btab[2],btab[3]);
}


float16 load_4x4_tile_and_transform(__global const float * restrict img,int r,int c)
{
    float4 vals[4];
    #pragma unroll
    for(int dr=0;dr<4;dr++,img+=IMG_W) {
        if(r+dr >= 0 && r+dr < IMG_H) { 
            if(c>= 0 && c + 3 < IMG_W)
                vals[dr] = vload4(0,img);
            else {
                vals[dr].s0 = (c+0 >= 0 && c+0 < IMG_W) ? img[0] : 0;
                vals[dr].s1 = (c+1 >= 0 && c+1 < IMG_W) ? img[1] : 0;
                vals[dr].s2 = (c+2 >= 0 && c+2 < IMG_W) ? img[2] : 0;
                vals[dr].s3 = (c+3 >= 0 && c+3 < IMG_W) ? img[3] : 0;
            }
        }
        else {
            vals[dr] = 0;
        }
    }

    

    return transform_tile(vals);
}


float16 load_2x2_tile_and_transform(__global const float * restrict img,int r,int c)
{
    float4 v;
    if(r < IMG_H) {
        v.s0 = c+0 < IMG_W ? img[0] : 0.0;
        v.s1 = c+1 < IMG_W ? img[1] : 0.0;
    }
    else {
        v.lo = 0;
    }
    if(r + 1 < IMG_H) {
        img += IMG_W;
        v.s2 = c+0 < IMG_W ? img[0] : 0.0;
        v.s3 = c+1 < IMG_W ? img[1] : 0.0;
    }
    else {
        v.hi = 0;
    }

    return tile2x2_to_4x4(v);
}


void transform_kernel_bwd(float16 v,float gtkg[9])
{
   //
    // G' = 1   0.5   0.5  0
    //      0   0.5  -0.5  0
    //      0   0.5   0.5  1 
    // (G'*k) * G

    float4 gtk[3];
    float4 k[4]= { v.lo.lo, v.lo.hi, v.hi.lo, v.hi.hi }; 

    gtk[0] = k[0] + (float4)(0.5) * (k[1] + k[2]);
    gtk[1] = (float4)(0.5) * (k[1] - k[2]);
    gtk[2] = (float4)(0.5) * (k[1] + k[2]) + k[3];

    // G = 
    // [[ 1.   0.   0. ]
    //  [ 0.5  0.5  0.5]
    //  [ 0.5 -0.5  0.5]
    //  [ 0.   0.   1. ]]
  
    #pragma unroll
    for(int i=0,p=0;i<3;i++,p+=3) {
        gtkg[p+0] = gtk[i].s0 + 0.5 * (gtk[i].s1 + gtk[i].s2);
        gtkg[p+1] = 0.5 * (gtk[i].s1 - gtk[i].s2);
        gtkg[p+2] = 0.5 * (gtk[i].s1 + gtk[i].s2) + gtk[i].s3;
    }
}


inline void store_local(__local float *l_val,int strd,float16 v)
{
        l_val[ 0*strd] = v.s0;
        l_val[ 1*strd] = v.s1;
        l_val[ 2*strd] = v.s2;
        l_val[ 3*strd] = v.s3;
        l_val[ 4*strd] = v.s4;
        l_val[ 5*strd] = v.s5;
        l_val[ 6*strd] = v.s6;
        l_val[ 7*strd] = v.s7;
        l_val[ 8*strd] = v.s8;
        l_val[ 9*strd] = v.s9;
        l_val[10*strd] = v.sa;
        l_val[11*strd] = v.sb;
        l_val[12*strd] = v.sc;
        l_val[13*strd] = v.sd;
        l_val[14*strd] = v.se;
        l_val[15*strd] = v.sf;
}

#define WG_SIZE 256
#define XTILES_IN_WG 32
#define YTILES_IN_WG 32
#define WG_K 8

#if  WG_K * XTILES_IN_WG  != WG_SIZE || YTILES_IN_WG * WG_K != WG_SIZE
#error "Parameters do not match"
#endif

#define WG_DIM 0

#define PAD_H 1
#define PAD_W 1

#define PATCH_Y 8
#define PATCH_X 8

#if XTILES_IN_WG * YTILES_IN_WG * 16 != PATCH_Y * PATCH_X * WG_SIZE
#error
#endif

#ifndef TR_STRIDE_OFFSET
#define TR_STRIDE_OFFSET 1
#endif

#if STRIDE_OFFSET > 0 || TR_STRIDE_OFFSET > 0
#define PADDING_FACTOR 1
#else
#define PADDING_FACTOR 0
#endif


__kernel 
__attribute__((reqd_work_group_size(WG_SIZE,1,1)))
void winconv_3x3_bwd_filter(int B, int N,int C,
                          __global float const * restrict image,ulong image_offset,
                          __global float * restrict kernels,ulong kernels_offset,
                          __global float const *restrict result,ulong result_offset,
                          float beta
                          )
{
    image += image_offset;
    kernels += kernels_offset;
    result += result_offset;

    #define half_W  ((IMG_W+1)/2)
    #define half_H  ((IMG_H+1)/2)
    #define half_WG (half_W * half_H)

    __local float wg_local_memory[(XTILES_IN_WG + YTILES_IN_WG + 16 * PADDING_FACTOR) * WG_K * 16];

#define l_xtile_stride (XTILES_IN_WG * WG_K + STRIDE_OFFSET)
#define l_ytile_stride (YTILES_IN_WG * WG_K + STRIDE_OFFSET)

#define l_xtiles(a,b,c) wg_local_memory[(a)*l_xtile_stride + (b)*XTILES_IN_WG + (c)]
#define l_ytiles(a,b,c) wg_local_memory[(a)*l_ytile_stride + (b)*YTILES_IN_WG + (c) + (XTILES_IN_WG*WG_K*16 + 32 * STRIDE_OFFSET)]


    // Loading data
    int l_xtile_c = get_local_id(WG_DIM) % XTILES_IN_WG;
    int l_xtile_k = get_local_id(WG_DIM) / XTILES_IN_WG;

    int l_ytile_n = get_local_id(WG_DIM) % YTILES_IN_WG;
    int l_ytile_k = get_local_id(WG_DIM) / YTILES_IN_WG;
   
    int wg_ch_in  = get_group_id(0) * XTILES_IN_WG;
    int wg_ch_out = get_global_id(1) * YTILES_IN_WG; 

    #define s_img_tile(k,t,indx) wg_local_memory[(k)*(XTILES_IN_WG/2*16 + TR_STRIDE_OFFSET) + (t/2) * 16 + (indx)]
    
    __local float *l_xtile_ptr = &l_xtiles(0,l_xtile_k,l_xtile_c);
    __local float *l_ytile_ptr = &l_ytiles(0,l_ytile_k,l_ytile_n);

    // GEMM

    int my_gemm_tile_b  = get_local_id(WG_DIM) / 16;
    int my_gemm_tile_tk = get_local_id(WG_DIM) % 16;
    int my_gemm_tile_kr = my_gemm_tile_tk / (XTILES_IN_WG / PATCH_X) * PATCH_Y;
    int my_gemm_tile_tl = my_gemm_tile_tk % (XTILES_IN_WG / PATCH_X) * PATCH_X;

    float p_C[PATCH_Y][PATCH_X]={{0}};

    int k=0;
    int K_limit = B * half_WG;
    bool reduce_k=false;
    if(get_global_size(2) != 1) {
        int K_size = (K_limit + get_global_size(2) - 1) / get_global_size(2);
        k = K_size * get_global_id(2);
        K_limit = min(K_limit,k + K_size);
        reduce_k = true;
    }
    
    image  += (wg_ch_in  + l_xtile_c) * (IMG_H * IMG_W);
    result += (wg_ch_out + l_ytile_n) * (IMG_H * IMG_W);

    for(;k < K_limit;k+=WG_K)
    {
        {
            int my_k = k + l_xtile_k;
            int batch  = my_k / half_WG;
            int rowcol = my_k % half_WG;
            int row    = rowcol / half_W * 2;
            int col    = rowcol % half_W * 2;
            __global float const *x = image + batch * C * IMG_H * IMG_W + (row - 1) * IMG_W + (col - 1);
            // load relevant tile       
            float16 my_xtile = (k + l_xtile_k < K_limit && l_xtile_c + wg_ch_in < C) ? load_4x4_tile_and_transform(x,row-1,col-1) : 0; 
            store_local(l_xtile_ptr,l_xtile_stride,my_xtile);
        }
        {
            int my_k = k + l_ytile_k;
            int batch  = my_k / half_WG;
            int rowcol = my_k % half_WG;
            int row    = rowcol / half_W * 2;
            int col    = rowcol % half_W * 2;
             __global float const *dy = result + batch * N * IMG_H * IMG_W + row * IMG_W + col;
            // load relevant kernel
            float16 my_ytile = (k + l_ytile_k < K_limit && l_ytile_n + wg_ch_out < N) ? load_2x2_tile_and_transform(dy,row,col) : 0;
            store_local(l_ytile_ptr,l_ytile_stride,my_ytile);
        }

        barrier(CLK_LOCAL_MEM_FENCE);

        // GEMM
        #pragma unroll
        for(int dk=0;dk<WG_K;dk++) {
            float p_y[PATCH_Y];

            #pragma unroll
            for(int dr=0;dr<PATCH_Y;dr++) {
                p_y[dr] = l_ytiles(my_gemm_tile_b,dk,dr + my_gemm_tile_kr);
            }
            float p_x[PATCH_X];
            #pragma unroll
            for(int dc=0;dc<PATCH_X;dc++) {
                p_x[dc] = l_xtiles(my_gemm_tile_b,dk,dc + my_gemm_tile_tl);
            }
            #pragma unroll
            for(int dr=0;dr<PATCH_Y;dr++) {
                #pragma unroll
                for(int dc=0;dc<PATCH_X;dc++) {
                    p_C[dr][dc] += p_y[dr]*p_x[dc];
                }
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    int tile0 = get_local_id(WG_DIM) * 4;
    int s_col_t0 = tile0 % XTILES_IN_WG;
    int s_row_t = tile0 / XTILES_IN_WG;

    #pragma unroll
    for(int dc_split = 0; dc_split < 2;dc_split++) {
        // transpose part A

        #pragma unroll
        for(int dr=0;dr<PATCH_Y;dr++) {
            int tile_r = dr + my_gemm_tile_kr;
            #pragma unroll
            for(int dc=0;dc<PATCH_X;dc+=2) {
                int tile_c = dc + dc_split + my_gemm_tile_tl;
                s_img_tile(tile_r,tile_c,my_gemm_tile_b) = p_C[dr][dc + dc_split];
             }
        }
        
        barrier(CLK_LOCAL_MEM_FENCE);

        #pragma unroll
        for(int dc = 0;dc < 4;dc+=2) {
            
            int s_col_t = s_col_t0 + dc + dc_split;

            int kern_ch_out = s_row_t + wg_ch_out;
            int kern_ch_in  = s_col_t + wg_ch_in;

            if(kern_ch_out >= N || kern_ch_in >= C)
                continue;

            float16 s_kern16 = vload16(0,&s_img_tile(s_row_t,s_col_t,0));
            float s_kern9[9];
            transform_kernel_bwd(s_kern16,s_kern9);
            __global float *ptr = kernels + 9* (kern_ch_in +  kern_ch_out * C);
            if(reduce_k) {
                #pragma unroll
                for(int i=0;i<9;i++) {
                    atomic_addf(ptr + i, s_kern9[i]);
                }
            }
            else {
                #pragma unroll
                for(int i=0;i<9;i++) {
                    if(beta == 0)
                        ptr[i] = s_kern9[i];
                    else
                        ptr[i] = ptr[i] * beta + s_kern9[i];
                }
            }
        }
        
        barrier(CLK_LOCAL_MEM_FENCE);
    }

}


)kern_src" },{"winograd_fwd", R"kern_src(///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
///
/// Copyright (c) 2021-2022 Artyom Beilis <artyomtnk@yahoo.com>
///
/// MIT License, see LICENSE.TXT
///
///////////////////////////////////////////////////////////////////////////////
#define ACTIVATION_IDENTITY 0
#define ACTIVATION_RELU     1
#define ACTIVATION_TANH     2
#define ACTIVATION_SIGMOID  3
#define ACTIVATION_RELU6    4

#ifndef dtype
#define dtype float
#define dtype2 float2
#define dtype4 float4
#define DTYPE_MAX FLT_MAX
#define DTYPE_MIN FLT_MIN
#endif


#ifndef ACTIVATION
#define ACTIVATION ACTIVATION_IDENTITY
#endif

#if ACTIVATION == ACTIVATION_IDENTITY
#   define ACTIVATION_F(x) (x)
#   define ACTIVATION_FINV(y,dy) (dy)
#   define ACTIVATION_NAME identity
#elif ACTIVATION == ACTIVATION_RELU
#   define ACTIVATION_F(x) (max((x),(dtype)(0)))
#   define ACTIVATION_FINV(y,dy)  ((y>0)?dy:0)
#   define ACTIVATION_NAME relu
#elif ACTIVATION == ACTIVATION_TANH
#   define ACTIVATION_F(x) (tanh((x)))
#   define ACTIVATION_FINV(y,dy) ((1-(y)*(y))*(dy))
#   define ACTIVATION_NAME tanh 
#elif ACTIVATION == ACTIVATION_SIGMOID
#   define ACTIVATION_F(x) ((dtype)(1) / ((dtype)(1) + exp(-(x))))
#   define ACTIVATION_FINV(y,dy) ((y)*(1-(y))*(dy))
#   define ACTIVATION_NAME sigmoid
#elif ACTIVATION == ACTIVATION_RELU6
#   define ACTIVATION_F(x) (min(max((x),(dtype)(0)),(dtype)(6)))
#   define ACTIVATION_FINV(y,dy)  ((0<y && y<6)?dy:0)
#   define ACTIVATION_NAME relu6
#else
#   error "Unknown activation"
#endif 



/***
 * Implementation concept is based on the
 *
 * Yan, Da, Wei Wang, and Xiaowen Chu.
 * "Optimizing batched winograd convolution on GPUs."
 * Proceedings of the 25th ACM SIGPLAN symposium on
 * principles and practice of parallel programming. 2020.
 *
 * @inproceedings{yan2020optimizing,
 *      title={Optimizing batched winograd convolution on GPUs},
 *      author={Yan, Da and Wang, Wei and Chu, Xiaowen},
 *      booktitle={Proceedings of the 25th ACM SIGPLAN symposium on principles and practice of parallel programming},
 *      pages={32--44},
 *      year={2020}
 * }
 *
 *
 * https://www.cse.ust.hk/~weiwa/papers/yan-ppopp20.pdf
 *
 * In comparison to the paper: using 32x8 tiles block instead of 64x8
 * since it generic OpenCL implementation for different GPUs
 * that can't optimize registers as efficienlty as manually written
 * assembly
 */

float16 transform_tile(float4 a[4])
{
    float bta[4][4];

    bta[0][0] = a[0].s0 - a[2].s0;
    bta[0][1] = a[0].s1 - a[2].s1;
    bta[0][2] = a[0].s2 - a[2].s2;
    bta[0][3] = a[0].s3 - a[2].s3;

    bta[1][0] = a[1].s0 + a[2].s0;
    bta[1][1] = a[1].s1 + a[2].s1;
    bta[1][2] = a[1].s2 + a[2].s2;
    bta[1][3] = a[1].s3 + a[2].s3;

    bta[2][0] = a[2].s0 - a[1].s0;
    bta[2][1] = a[2].s1 - a[1].s1;
    bta[2][2] = a[2].s2 - a[1].s2;
    bta[2][3] = a[2].s3 - a[1].s3;

    bta[3][0] = a[1].s0 - a[3].s0;
    bta[3][1] = a[1].s1 - a[3].s1;
    bta[3][2] = a[1].s2 - a[3].s2;
    bta[3][3] = a[1].s3 - a[3].s3;

    float4 btab[4];
    #pragma unroll
    for(int i=0;i<4;i++) {
        btab[i].s0 = bta[i][0] - bta[i][2];
        btab[i].s1 = bta[i][1] + bta[i][2];
        btab[i].s2 = bta[i][2] - bta[i][1];
        btab[i].s3 = bta[i][1] - bta[i][3];
    }
    
    return (float16)(btab[0],btab[1],btab[2],btab[3]);
}


float16 load_4x4_tile_and_transform_v2(__global const float * restrict frame,int pos,int limit,int end,int stride,int4 mask[4])
{
    float4 a[4];
    frame += pos;
    
    if(pos >= 0 && pos < limit) {
        #pragma unroll
        for(int i=0;i<4;i++,frame+=stride) {
            a[i] = as_float4(as_int4(vload4(0,frame)) & mask[i]);
        }
    }
    else {
        #pragma unroll
        for(int i=0;i<4;i++,frame+=stride,pos+=stride) {
            if(pos >= 0 && pos + 4 <= end)
               a[i] = as_float4(as_int4(vload4(0,frame)) & mask[i]);
            else {
                float4 tmp;
                tmp.s0 = pos+0 >= 0 && pos+0 < end ? frame[0] : 0.0;
                tmp.s1 = pos+1 >= 0 && pos+1 < end ? frame[1] : 0.0;
                tmp.s2 = pos+2 >= 0 && pos+2 < end ? frame[2] : 0.0;
                tmp.s3 = pos+3 >= 0 && pos+3 < end ? frame[3] : 0.0;
                a[i] = as_float4(as_int4(tmp) & mask[i]);
            }
        }
    }
    return transform_tile(a);
}



float16 load_3x3_kernel_and_transform(__global const float *kern_ptr)
{
    float4 gk[3];
    float k[9];
    
    #pragma unroll
    for(int i=0;i<9;i++)
        k[i]=kern_ptr[i];

    gk[0].s0 = k[0];
    gk[1].s0 = k[1];
    gk[2].s0 = k[2];

    gk[0].s1 = 0.5f * (k[0] + k[3] + k[6]);
    gk[1].s1 = 0.5f * (k[1] + k[4] + k[7]);
    gk[2].s1 = 0.5f * (k[2] + k[5] + k[8]);

    gk[0].s2 = 0.5f * (k[0] - k[3] + k[6]);
    gk[1].s2 = 0.5f * (k[1] - k[4] + k[7]);
    gk[2].s2 = 0.5f * (k[2] - k[5] + k[8]);

    gk[0].s3 = k[6];
    gk[1].s3 = k[7];
    gk[2].s3 = k[8];

    float16 k4;

    k4.s048c = gk[0];
    k4.s159d = 0.5f * (gk[0] + gk[1] + gk[2]);
    k4.s26ae = 0.5f * (gk[0] - gk[1] + gk[2]);
    k4.s37bf = gk[2];
    return k4;
}


float4 tile4x4_after_wingorad_to_2x2(float16 tile)
{
    float4 p0 = tile.s0123;
    float4 p1 = tile.s4567;
    float4 p2 = tile.s89ab;
    float4 p3 = tile.scdef;

    float4 Atp[2];
    Atp[0] = p0 + p1 + p2;
    Atp[1] = p1 - p2 - p3;
    
    float2 r[2];

    #pragma unroll
    for(int i=0;i<2;i++) {
        r[i].s0 = Atp[i].s0 + Atp[i].s1 + Atp[i].s2;
        r[i].s1 = Atp[i].s1 - Atp[i].s2 - Atp[i].s3;
    }

    return (float4)(r[0],r[1]);
}


__kernel void winconv_calc_gkgt_3x3(int N,int C,
                                    __global const float * restrict gk3,
                                    ulong gk3_offset,
                                    __global float16 *k4,
                                    ulong k4_offset)
{
    gk3 += gk3_offset;
    k4 += k4_offset / 16;
    int n = get_global_id(0);
    int c = get_global_id(1);
    if(n >= N || c>= C)
        return;
    float16 kern = load_3x3_kernel_and_transform(gk3 + (C * n + c) * 9);
    k4[C * n + c] = kern;
}



inline void store_local(__local float *l_val,int strd,float16 v)
{
        l_val[ 0*strd] = v.s0;
        l_val[ 1*strd] = v.s1;
        l_val[ 2*strd] = v.s2;
        l_val[ 3*strd] = v.s3;
        l_val[ 4*strd] = v.s4;
        l_val[ 5*strd] = v.s5;
        l_val[ 6*strd] = v.s6;
        l_val[ 7*strd] = v.s7;
        l_val[ 8*strd] = v.s8;
        l_val[ 9*strd] = v.s9;
        l_val[10*strd] = v.sa;
        l_val[11*strd] = v.sb;
        l_val[12*strd] = v.sc;
        l_val[13*strd] = v.sd;
        l_val[14*strd] = v.se;
        l_val[15*strd] = v.sf;
}

#define WG_SIZE 256
#define TILES_IN_WG 32
#define KERNELS_IN_WG 32
#define WG_K 8

#if  WG_K * TILES_IN_WG  != WG_SIZE || KERNELS_IN_WG * WG_K != WG_SIZE
#error "Parameters do not match"
#endif

#define WG_DIM 0

#define PAD_H 1
#define PAD_W 1

#define PATCH_K 8
#define PATCH_T 8

void scale_and_add(__global float *p,float value,float scale)
{
    if(scale == 0.0)
        *p = value;
    else
        *p = *p * scale + value;
}

#if TILES_IN_WG * KERNELS_IN_WG * 16 != PATCH_K * PATCH_T * WG_SIZE
#error
#endif
#ifndef BIAS
#define BIAS 0
#endif

#ifndef TR_STRIDE_OFFSET
#define TR_STRIDE_OFFSET 1
#endif

#if STRIDE_OFFSET > 0 || TR_STRIDE_OFFSET > 0
#define PADDING_FACTOR 1
#else
#define PADDING_FACTOR 0
#endif

__kernel 
__attribute__((reqd_work_group_size(WG_SIZE,1,1)))
void winconv_3x3(int B, int N,int C,int H,int W,
                  __global float const * restrict image,ulong image_offset,
                  __global float16 const * restrict kernels,ulong kernels_offset,
#if BIAS == 1                  
                  __global float const * restrict bias,ulong bias_offset,
#endif                  
                  __global float *restrict result,ulong result_offset,
                  float scale)
{
    image += image_offset;
    kernels += kernels_offset / 16;
#if BIAS == 1                  
    bias += bias_offset;
#endif    
    result += result_offset;

    int half_W = (W+1)/2;
    int half_H = (H+1)/2;
    int half_WG = half_W * half_H;

    __local float wg_local_memory[(TILES_IN_WG + KERNELS_IN_WG + (16*PADDING_FACTOR)) * WG_K * 16];

#define l_tile_stride (TILES_IN_WG * WG_K + STRIDE_OFFSET)
#define l_kern_stride (KERNELS_IN_WG * WG_K + STRIDE_OFFSET)

#define l_tiles(a,b,c) wg_local_memory[(a)*l_tile_stride + (b)*TILES_IN_WG + (c)]
#define l_kernels(a,b,c) wg_local_memory[(a)*l_kern_stride + (b)*KERNELS_IN_WG + (c) + (TILES_IN_WG*WG_K*16 + (32*STRIDE_OFFSET))]


    // Loading data
    int l_tile_rc = get_local_id(WG_DIM) % TILES_IN_WG;
    int l_tile_k  = get_local_id(WG_DIM) / TILES_IN_WG;

    int l_kern_n  = get_local_id(WG_DIM) % KERNELS_IN_WG;
    int l_kern_k  = get_local_id(WG_DIM) / KERNELS_IN_WG;
   
    int wg_brc    = get_group_id(0) * TILES_IN_WG;
    int wg_feature= get_global_id(1) * KERNELS_IN_WG; 

    int l_brc     = wg_brc + l_tile_rc;
    int l_feature = wg_feature + l_kern_n;

    int l_b   = l_brc / half_WG;
    int l_rc  = l_brc % half_WG;
    int l_row = l_rc / half_W * 2;
    int l_col = l_rc % half_W * 2;

    int image_pos = l_b * H * W * C + (l_row - 1) * W + l_col - 1 + (l_tile_k) * (H * W);
    int image_end   = B * C * W * H;
    int image_step  = H*W*WG_K;
    int image_limit = image_end - W*4;
    int4 load_mask[4];
    #pragma unroll
    for(int dr=0;dr<4;dr++) {
        if(l_row - 1 + dr < 0 || l_row - 1 + dr >= H) {
            load_mask[dr] = 0;
        }
        else {
            int c=l_col - 1;
            load_mask[dr].s0 = c >= 0 && c < W ? -1 : 0;
            c++;
            load_mask[dr].s1 = c >= 0 && c < W ? -1 : 0;
            c++;
            load_mask[dr].s2 = c >= 0 && c < W ? -1 : 0;
            c++;
            load_mask[dr].s3 = c >= 0 && c < W ? -1 : 0;
            c++;
        }
    }

    #define s_img_tile(k,t,indx) wg_local_memory[(k)*(TILES_IN_WG/2*16 + TR_STRIDE_OFFSET) + (t/2) * 16 + (indx)]
    
    __local float *l_tile_ptr = &l_tiles(0,l_tile_k,l_tile_rc);
    __local float *l_kern_ptr = &l_kernels(0,l_kern_k,l_kern_n);

    // GEMM

    int my_gemm_tile_b  = get_local_id(WG_DIM) / 16;
    int my_gemm_tile_tk = get_local_id(WG_DIM) % 16;
    int my_gemm_tile_kr = my_gemm_tile_tk / (TILES_IN_WG / PATCH_T) * PATCH_K;
    int my_gemm_tile_tl = my_gemm_tile_tk % (TILES_IN_WG / PATCH_T) * PATCH_T;

    float p_C[PATCH_K][PATCH_T]={{0}};

    // store

    for(int k=0;k < C;k+=WG_K,image_pos += WG_K*W*H)
    {

        // load relevant tile        
        float16 my_tile = (l_b < B && k + l_tile_k < C) ? load_4x4_tile_and_transform_v2(image,image_pos,image_limit,image_end,W,load_mask) : 0; 
        store_local(l_tile_ptr,l_tile_stride,my_tile);
        
        // load relevant kernel
        float16 my_kern = (l_feature < N && k+l_kern_k < C) ? kernels[l_feature * C + k + l_kern_k] : 0;
        store_local(l_kern_ptr,l_kern_stride,my_kern);

        barrier(CLK_LOCAL_MEM_FENCE);

        // GEMM
        #pragma unroll
        for(int dk=0;dk<WG_K;dk++) {
            float p_kern[PATCH_K];

            #pragma unroll
            for(int dr=0;dr<PATCH_K;dr++) {
                p_kern[dr] = l_kernels(my_gemm_tile_b,dk,dr + my_gemm_tile_kr);
            }
            float p_tile[PATCH_T];
            #pragma unroll
            for(int dc=0;dc<PATCH_T;dc++) {
                p_tile[dc] = l_tiles(my_gemm_tile_b,dk,dc + my_gemm_tile_tl);
            }
            #pragma unroll
            for(int dr=0;dr<PATCH_K;dr++) {
                #pragma unroll
                for(int dc=0;dc<PATCH_T;dc++) {
                    p_C[dr][dc] += p_kern[dr]*p_tile[dc];
                }
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    int tile0 = get_local_id(WG_DIM) * 4;
    int s_col_t0 = tile0 % TILES_IN_WG;
    int s_row_t = tile0 / TILES_IN_WG;

    #pragma unroll
    for(int dc_split = 0; dc_split < 2;dc_split++) {
        // transpose part A

        #pragma unroll
        for(int dr=0;dr<PATCH_K;dr++) {
            int tile_r = dr + my_gemm_tile_kr;
            #pragma unroll
            for(int dc=0;dc<PATCH_T;dc+=2) {
                int tile_c = dc + dc_split + my_gemm_tile_tl;
                s_img_tile(tile_r,tile_c,my_gemm_tile_b) = p_C[dr][dc + dc_split];
             }
        }
        
        barrier(CLK_LOCAL_MEM_FENCE);

        #pragma unroll
        for(int dc = 0;dc < 4;dc+=2) {
            
            int s_col_t = s_col_t0 + dc + dc_split;

            float16 s_tile = vload16(0,&s_img_tile(s_row_t,s_col_t,0));
            float4 s_img_tile = tile4x4_after_wingorad_to_2x2(s_tile);

            int s_brc = wg_brc + s_col_t;

            int s_b   = s_brc / half_WG;
            int s_rc  = s_brc % half_WG;
            int s_row = s_rc / half_W * 2;
            int s_col = s_rc % half_W * 2;

            int s_feature = wg_feature + s_row_t;

            __global float *ptr = result + ((s_b * N + s_feature) * H + s_row) * W + s_col;

            #if BIAS == 1
            float bias_v = s_feature < N ? bias[s_feature] : 0.0f;
            s_img_tile += (float4)(bias_v);
            #endif

            s_img_tile.s0 = ACTIVATION_F(s_img_tile.s0);
            s_img_tile.s1 = ACTIVATION_F(s_img_tile.s1);
            s_img_tile.s2 = ACTIVATION_F(s_img_tile.s2);
            s_img_tile.s3 = ACTIVATION_F(s_img_tile.s3);


            if(s_b < B && s_feature < N) {
                bool rv0 = s_row < H;
                bool rv1 = s_row + 1 < H;
                bool cv0 = s_col < W;
                bool cv1 = s_col + 1 < W;
                if(rv0) {
                    if(cv0)
                        scale_and_add(ptr + 0, s_img_tile.s0, scale);
                    if(cv1)
                        scale_and_add(ptr + 1, s_img_tile.s1, scale);
                }
                ptr += W;
                if(rv1) {
                    if(cv0)
                        scale_and_add(ptr + 0, s_img_tile.s2, scale);
                    if(cv1)
                        scale_and_add(ptr + 1, s_img_tile.s3, scale);
                }
            }
        }
        
        barrier(CLK_LOCAL_MEM_FENCE);
    }

}

)kern_src" },};
}} // namespaces
